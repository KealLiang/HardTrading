import logging
import os
import warnings


from bin import simulator
from bin.resilience_scanner import run_filter
from bin.scanner_analyzer import scan_and_visualize_analyzer
from strategy.breakout_strategy import BreakoutStrategy
from strategy.breakout_strategy_v2 import BreakoutStrategyV2
from strategy.scannable_strategy import ScannableBreakoutStrategy
from strategy.hybrid_strategy import HybridStrategy
from strategy.market_regime import MarketRegimeStrategy
from strategy.origin_breakout_strategy import OriginBreakoutStrategy
from strategy.panic_rebound_strategy import PanicReboundStrategy
from strategy.pullback_rebound_strategy import PullbackReboundStrategy
from strategy.scannable_pullback_rebound_strategy import ScannablePullbackReboundStrategy
from strategy.regime_classifier_strategy import RegimeClassifierStrategy

from utils.logging_util import redirect_print_to_logger

# å¿½ç•¥jiebaåº“ä¸­çš„pkg_resourcesè­¦å‘Š
warnings.filterwarnings("ignore", category=DeprecationWarning)

from datetime import datetime
from analysis.calculate_limit_up_success_rate import analyze_rate
from analysis.daily_group import find_stocks_by_hot_themes
from analysis.dejavu import process_dejavu_data
from analysis.fupan_statistics import fupan_all_statistics
from analysis.fupan_statistics_plot import plot_all
from analysis.seek_historical_similar import find_other_similar_trends
from analysis.stock_price_plotter import plot_multiple_stocks
from analysis.time_price_sharing import analyze_abnormal_stocks_time_sharing
from analysis.whimsical import process_zt_data
from analysis.ladder_chart import build_ladder_chart
from fetch.astock_concept import fetch_and_save_stock_concept
from fetch.astock_data import StockDataFetcher
from fetch.astock_data_minutes import fetch_and_save_stock_data
from fetch.indexes_data import fetch_indexes_data
from fetch.lhb_data import fetch_and_merge_stock_lhb_detail, fetch_and_filter_yybph_lhb_data, fetch_yyb_lhb_data, \
    find_top_yyb_trades
from fetch.tonghuashun.fupan import all_fupan
from fetch.tonghuashun.fupan_plot import draw_fupan_lb
from fetch.tonghuashun.hotpoint_analyze import hot_words_cloud

from filters.find_abnormal import find_serious_abnormal_stocks_range
from filters.find_longtou import find_dragon_stocks
from utils.synonym_manager import SynonymManager
from bin.experiment_runner import run_comparison_experiment
from bin.psq_analyzer import run_psq_analysis_report
from bin.parameter_optimizer import ParameterOptimizer

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(threadName)s] %(levelname)s - %(message)s')


def run_psq_analysis():
    """
    ä¸€é”®åŒ–PSQç»¼åˆåˆ†ææŠ¥å‘Šçš„æ–°å…¥å£
    """
    run_psq_analysis_report()


def run_parameter_optimization(config_name=None):
    """
    è¿è¡Œå‚æ•°ä¼˜åŒ–

    Args:
        config_name: é…ç½®æ–‡ä»¶åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    """
    optimizer = ParameterOptimizer()

    if config_name is None:
        # ç”Ÿæˆé»˜è®¤é…ç½®æ¨¡æ¿
        print("æœªæŒ‡å®šé…ç½®æ–‡ä»¶ï¼Œç”Ÿæˆé»˜è®¤é…ç½®æ¨¡æ¿...")
        template_path = optimizer.generate_config_template("default")
        print(f"é»˜è®¤é…ç½®æ¨¡æ¿å·²ç”Ÿæˆ: {template_path}")
        print("è¯·ç¼–è¾‘é…ç½®æ–‡ä»¶åé‡æ–°è¿è¡Œ")
        return template_path
    else:
        # è¿è¡Œä¼˜åŒ–
        config_path = f"bin/optimization_configs/{config_name}"
        if not config_name.endswith('.yaml'):
            config_path += '.yaml'

        if not os.path.exists(config_path):
            print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return None

        print(f"å¼€å§‹è¿è¡Œå‚æ•°ä¼˜åŒ–ï¼Œé…ç½®æ–‡ä»¶: {config_path}")
        report_path = optimizer.run_optimization(config_path)
        print(f"å‚æ•°ä¼˜åŒ–å®Œæˆï¼æŠ¥å‘Šä¿å­˜åœ¨: {report_path}")
        return report_path


def generate_optimization_templates():
    """
    ç”Ÿæˆå„ç§ç±»å‹çš„ä¼˜åŒ–é…ç½®æ¨¡æ¿
    """
    optimizer = ParameterOptimizer()

    templates = {
        "default": "é»˜è®¤é…ç½®æ¨¡æ¿",
        "quick": "å¿«é€Ÿæµ‹è¯•æ¨¡æ¿",
        "grid": "ç½‘æ ¼æœç´¢æ¨¡æ¿",
        "compare": "å‚æ•°æ–‡ä»¶å¯¹æ¯”æ¨¡æ¿"
    }

    generated_files = []
    for template_type, description in templates.items():
        if template_type == "compare":
            # compareæ¨¡æ¿ä¸éœ€è¦é¢å¤–å‚æ•°
            template_path = optimizer.generate_config_template(template_type=template_type)
        else:
            template_path = optimizer.generate_config_template(template_type=template_type,
                                                               strategy_class=BreakoutStrategy,
                                                               test_params=['consolidation_lookback',
                                                                            'consolidation_ma_proximity_pct',
                                                                            'consolidation_ma_max_slope'])
        generated_files.append(template_path)
        print(f"{description}å·²ç”Ÿæˆ: {template_path}")

    print(f"\næ€»å…±ç”Ÿæˆäº† {len(generated_files)} ä¸ªé…ç½®æ¨¡æ¿")
    print("è¯·æ ¹æ®éœ€è¦ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œç„¶åè¿è¡Œå‚æ•°ä¼˜åŒ–")

    return generated_files


# å›æº¯äº¤æ˜“
def backtrade_simulate():
    # æ‰¹é‡å›æµ‹å¹¶å¯¹æ¯”
    # run_comparison_experiment()

    # å•ä¸ªå›æµ‹
    stock_code = '300059'
    simulator.go_trade(
        code=stock_code,
        amount=100000,
        startdate=datetime(2022, 1, 1),
        enddate=datetime(2025, 8, 22),
        strategy=BreakoutStrategyV2,
        strategy_params={'debug': True},  # å¼€å¯è¯¦ç»†æ—¥å¿—
        log_trades=True,
        visualize=True,
        interactive_plot=True,  # å¼¹å‡ºäº¤äº’å›¾
    )


# æ­¢è·Œåå¼¹ç­–ç•¥å›æµ‹
def pullback_rebound_simulate():
    """æ­¢è·Œåå¼¹ç­–ç•¥å›æµ‹ç¤ºä¾‹"""
    stock_code = '603986'  # å¯ä»¥æ›¿æ¢ä¸ºå…¶ä»–è‚¡ç¥¨ä»£ç 
    simulator.go_trade(
        code=stock_code,
        amount=100000,
        startdate=datetime(2023, 9, 25),  # ä¿®æ­£æ—¶é—´èŒƒå›´
        enddate=datetime(2025, 9, 17),
        strategy=PullbackReboundStrategy,
        strategy_params={
            'debug': False,  # å…³é—­è°ƒè¯•æ—¥å¿—ï¼Œåªæ˜¾ç¤ºäº¤æ˜“æ—¥å¿—
            'uptrend_min_gain': 0.30,  # ä¿æŒ30%è¦æ±‚
            'volume_dry_ratio': 0.7,   # é‡çª’æ¯é˜ˆå€¼è°ƒæ•´ä¸º70%
        },
        log_trades=True,
        visualize=True,
        interactive_plot=True,  # å¼¹å‡ºäº¤äº’å›¾
    )


def strategy_scan(candidate_model='a'):
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„ä¿¡å·æ¨¡å¼åˆ—è¡¨
    signal_patterns = [
        # '*** è§¦å‘ã€çªç ´è§‚å¯Ÿå“¨ã€‘',
        # 'çªç ´ä¿¡å·',
        '*** äºŒæ¬¡ç¡®è®¤ä¿¡å·',
    ]

    start_date = '20250630'
    end_date = None
    stock_pool = ['300581', '600475']
    details_after_date = '20250910'  # åªçœ‹è¿™ä¸ªæ—¥æœŸä¹‹åçš„

    # æ‰«æä¸å¯è§†åŒ–
    scan_and_visualize_analyzer(
        scan_strategy=BreakoutStrategy,
        scan_start_date=start_date,
        scan_end_date=end_date,
        stock_pool=None,
        signal_patterns=signal_patterns,
        details_after_date=details_after_date,  # åªæœ‰æ­¤æ—¥æœŸåä¿¡å·æ‰è¾“å‡ºè¯¦æƒ…
        candidate_model=candidate_model,
        output_path='bin/candidate_stocks_breakout'  # æŒ‡å®šè¾“å‡ºç›®å½•
    )


def pullback_rebound_scan(candidate_model='a'):
    """æ­¢è·Œåå¼¹ç­–ç•¥æ‰«æ"""
    # ä½¿ç”¨æ­¢è·Œåå¼¹ç­–ç•¥çš„ä¿¡å·æ¨¡å¼
    signal_patterns = [
        '*** æ­¢è·Œåå¼¹ä¹°å…¥ä¿¡å·è§¦å‘',
        'æ­¢è·Œåå¼¹ä¿¡å·',
    ]

    start_date = '20250630'
    end_date = None
    details_after_date = '20250901'  # åªçœ‹è¿™ä¸ªæ—¥æœŸä¹‹åçš„

    # æ‰«æä¸å¯è§†åŒ–
    scan_and_visualize_analyzer(
        scan_strategy=ScannablePullbackReboundStrategy,
        scan_start_date=start_date,
        scan_end_date=end_date,
        stock_pool=None,
        signal_patterns=signal_patterns,
        details_after_date=details_after_date,  # åªæœ‰æ­¤æ—¥æœŸåä¿¡å·æ‰è¾“å‡ºè¯¦æƒ…
        candidate_model=candidate_model,
        output_path='bin/candidate_stocks_rebound'  # æŒ‡å®šè¾“å‡ºç›®å½•
    )


def find_candidate_stocks():
    run_filter()


# è·å–çƒ­ç‚¹æ¦‚å¿µè¯äº‘
def get_hot_clouds():
    hot_words_cloud(0)


def get_index_data():
    # æŒ‡å®šä¿å­˜ç›®å½•
    save_directory = "data/indexes"
    fetch_indexes_data(save_directory)


def execute_routine(steps, routine_name="è‡ªå®šä¹‰æµç¨‹"):
    """
    é€šç”¨çš„æµç¨‹æ‰§è¡Œå™¨

    Args:
        steps: æ­¥éª¤åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (function, description) æˆ– function
        routine_name: æµç¨‹åç§°ï¼Œç”¨äºæ—¥å¿—æ–‡ä»¶å‘½å
    """
    import time
    import threading
    import matplotlib

    # è®¾ç½®matplotlibä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé¿å…Tkinterçº¿ç¨‹é—®é¢˜
    matplotlib.use('Agg')

    # è·å–å½“å‰æ—¥æœŸç”¨äºæ—¥å¿—æ–‡ä»¶å‘½å
    current_date = datetime.now().strftime('%Y%m%d')
    routine_name_safe = routine_name.replace(" ", "_")
    log_filename = f"logs/{routine_name_safe}_{current_date}_{datetime.now().strftime('%H%M%S')}.log"

    # ç¡®ä¿logsç›®å½•å­˜åœ¨
    os.makedirs("logs", exist_ok=True)

    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_filename, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter('%(asctime)s - [%(threadName)s] %(levelname)s - %(message)s')
    file_handler.setFormatter(file_formatter)

    # è·å–æ ¹æ—¥å¿—è®°å½•å™¨å¹¶æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
    root_logger = logging.getLogger()
    root_logger.addHandler(file_handler)

    # åˆ›å»ºprintè¾“å‡ºé‡å®šå‘çš„logger
    # è®¾ç½®propagate=Falseé˜²æ­¢æ—¥å¿—å‘ä¸Šä¼ æ’­åˆ°root loggerï¼Œé¿å…é‡å¤è®°å½•
    print_logger = logging.getLogger('print_capture')
    print_logger.propagate = False
    print_logger.addHandler(file_handler)

    # åŒæ—¶åœ¨æ§åˆ¶å°æ˜¾ç¤ºç®€åŒ–ä¿¡æ¯
    print(f"=== å¼€å§‹{routine_name} {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===")
    print(f"è¯¦ç»†æ—¥å¿—ä¿å­˜åˆ°: {log_filename}")
    print(f"æ€»å…± {len(steps)} ä¸ªæ­¥éª¤")

    start_time = time.time()

    try:
        for i, step in enumerate(steps, 1):
            # è§£ææ­¥éª¤é…ç½®
            if isinstance(step, tuple):
                func, description = step
            else:
                func = step
                description = func.__name__

            step_start_time = time.time()

            print(f"\n[æ­¥éª¤{i}/{len(steps)}] å¼€å§‹{description}...")
            logging.info(f"=== æ­¥éª¤{i}: å¼€å§‹{description} ===")
            logging.info(f"å½“å‰ä¸»çº¿ç¨‹: {threading.current_thread().name}")
            logging.info(f"å½“å‰æ´»è·ƒçº¿ç¨‹æ•°: {threading.active_count()}")

            # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨é‡å®šå‘printåˆ°æ—¥å¿—æ–‡ä»¶ï¼ˆåªåœ¨æ­¥éª¤æ‰§è¡ŒæœŸé—´ï¼‰
            with redirect_print_to_logger(print_logger):
                # æ‰§è¡Œæ­¥éª¤
                func()

            step_duration = time.time() - step_start_time
            logging.info(f"=== æ­¥éª¤{i}: {description}å®Œæˆ (è€—æ—¶: {step_duration:.2f}ç§’) ===")
            logging.info(f"æ­¥éª¤å®Œæˆåæ´»è·ƒçº¿ç¨‹æ•°: {threading.active_count()}")
            print(f"âœ“ {description}å®Œæˆ (è€—æ—¶: {step_duration:.2f}ç§’)")

        total_duration = time.time() - start_time
        print(
            f"\n=== æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆï¼æ€»è€—æ—¶: {total_duration:.2f}ç§’ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===")
        logging.info(f"=== {routine_name}å…¨éƒ¨å®Œæˆ (æ€»è€—æ—¶: {total_duration:.2f}ç§’) ===")

    except Exception as e:
        error_msg = f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(f"\nâŒ {error_msg}")
        logging.error(error_msg, exc_info=True)
        raise
    finally:
        # ç§»é™¤æ–‡ä»¶å¤„ç†å™¨ï¼Œé¿å…é‡å¤æ·»åŠ 
        root_logger.removeHandler(file_handler)
        print_logger.removeHandler(file_handler)
        file_handler.close()
        print(f"\nè¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_filename}")


def daily_routine():
    """
    æ—¥å¸¸é‡åŒ–äº¤æ˜“æ•°æ®å¤„ç†æµç¨‹
    """
    # å®šä¹‰æ—¥å¸¸æµç¨‹æ­¥éª¤
    daily_steps = [
        (get_stock_datas, "æ‹‰å–Aè‚¡äº¤æ˜“æ•°æ®"),
        (fetch_ths_fupan, "æ‹‰å–çƒ­é—¨ä¸ªè‚¡æ•°æ®"),
        (whimsical_fupan_analyze, "æ‰§è¡Œé¢˜æåˆ†æ"),
        (generate_ladder_chart, "ç”Ÿæˆçƒ­é—¨è‚¡å¤©æ¢¯"),
        (draw_ths_fupan, "ç»˜åˆ¶æ¶¨è·Œé«˜åº¦å›¾"),
        (fupan_statistics_to_excel, "ç”Ÿæˆç»Ÿè®¡æ•°æ®"),
        (fupan_statistics_excel_plot, "ç”Ÿæˆç»Ÿè®¡å›¾è¡¨"),
        (auction_fengdan_analyze, "å¤ç›˜åˆ†æå°å•æ•°æ®"),
    ]

    execute_routine(daily_steps, "daily_routine")


# æ‹‰aè‚¡å†å²æ•°æ®
def get_stock_datas():
    stock_list = ["600610", "300033"]
    use_realtime = True

    # åˆ›å»ºAè‚¡æ•°æ®è·å–å¯¹è±¡ï¼ŒæŒ‡å®šæ‹‰å–çš„å¤©æ•°å’Œä¿å­˜è·¯å¾„
    data_fetcher = StockDataFetcher(start_date='20250630', end_date=None, save_path='./data/astocks',
                                    max_workers=8, stock_list=None, force_update=False, max_sleep_time=2000)

    # æ ¹æ®å‚æ•°é€‰æ‹©ä¸åŒçš„æ•°æ®è·å–æ–¹å¼
    if use_realtime:
        # ä½¿ç”¨å®æ—¶æ•°æ®æ¥å£æ›´æ–°å½“å¤©æ•°æ®
        data_fetcher.fetch_and_save_data_from_realtime()
    else:
        # ä½¿ç”¨å†å²æ•°æ®æ¥å£è·å–æ•°æ®ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        data_fetcher.fetch_and_save_data()

    # è·å–æŒ‡æ•°æ•°æ®
    get_index_data()


def get_stock_minute_datas():
    fetch_and_save_stock_data(
        interval="15",  # æ‹‰å– 15 åˆ†é’Ÿçº§åˆ«æ•°æ®
        start_date="20241110",  # èµ·å§‹æ—¥æœŸ
        end_date="20241210",  # ç»ˆæ­¢æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        # stock_list=["000717", "603776"],  # åªæ‹‰å–è¿™ä¸¤ä¸ªè‚¡ç¥¨çš„æ•°æ®
        output_dir="./data/astocks_minute"  # ä¿å­˜åˆ°æŒ‡å®šç›®å½•
    )


def fetch_and_filter_top_yybph():
    # ä½¿ç”¨ç¤ºä¾‹
    symbol = "è¿‘ä¸‰æœˆ"
    file_path = "./data/lhb/top_yybph.csv"  # ä¿å­˜çš„ CSV æ–‡ä»¶è·¯å¾„ï¼Œè¯·æ ¹æ®éœ€è¦ä¿®æ”¹

    fetch_and_filter_yybph_lhb_data(symbol, file_path)


def get_lhb_datas():
    start_date = "2024-08-01"
    end_date = None
    first_file_path = './data/lhb/yyb_lhb_data.csv'
    second_file_path = './data/lhb/stock_lhb_details.csv'
    trader_name = "ä¸­å›½é“¶æ²³è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸å¤§è¿é»„æ²³è·¯è¯åˆ¸è¥ä¸šéƒ¨"
    # 1. æ‹‰å–è¥ä¸šéƒ¨é¾™è™æ¦œæ•°æ®
    fetch_yyb_lhb_data(start_date, end_date, first_file_path)
    # 2. éå†è¥ä¸šéƒ¨æ•°æ®ï¼Œæ‹‰å–ä¸ªè‚¡é¾™è™æ¦œåˆå¹¶
    fetch_and_merge_stock_lhb_detail(first_file_path, second_file_path, trader_name)


def get_top_yyb_trades():
    # æ‰¾æœ€é¡¶çº§æ¸¸èµ„çš„äº¤æ˜“æ•°æ®
    # éœ€å…ˆè¿è¡Œfetch_and_filter_top_yybph()è·å–"top_yybph_lhb_data.csv"
    find_top_yyb_trades('./data/lhb/')


# æ‰¾é¾™å¤´
def find_dragon():
    start_date = '2025-01-01'
    # end_date = '2025-02-28'
    end_date = None
    find_dragon_stocks(start_date, end_date, threshold=180)


def find_yidong():
    # date = '2025-04-28'
    # find_serious_abnormal_stocks(date, check_updown_fluctuation=False)

    start_date = '2025-05-06'
    end_date = None
    find_serious_abnormal_stocks_range(start_date, end_date)


def get_stock_concept_and_industry():
    fetch_and_save_stock_concept(
        concept_list=["äº‘æ¸¸æˆ", "æ–°èƒ½æºè½¦"],
        industry_list=["é“¶è¡Œ", "æˆ¿åœ°äº§"],
        output_path="./excel/all_concepts.xlsx"
    )


def find_similar_trends():
    formatter = "%Y%m%d"
    data_dir = "./data/astocks"  # æ•°æ®æ–‡ä»¶æ‰€åœ¨ç›®å½•
    target_stock_code = "601068"  # ç›®æ ‡è‚¡ç¥¨ä»£ç 
    start_date = datetime.strptime('20250407', formatter)
    end_date = datetime.strptime('20250416', formatter)
    trend_end_date = datetime.strptime('20250618', formatter)  # è¢«æŸ¥æ‰¾ä¸ªè‚¡çš„è¶‹åŠ¿ç»“æŸæ—¥æœŸ

    # 1.å¯»æ‰¾è‡ªèº«ç›¸ä¼¼æ—¶æœŸ
    # target_index_code = "sz399001"  # ç›®æ ‡æŒ‡æ•°ä»£ç 
    # find_self_similar_windows(target_index_code, start_date, end_date, "./data/indexes", method="weighted")

    # 2.å¯»æ‰¾åŒæ—¶æœŸç›¸ä¼¼ä¸ªè‚¡
    # å¯é€‰è‚¡ç¥¨ä»£ç åˆ—è¡¨
    # stock_codes = [
    #     "600928",
    #     "601319",
    #     "001227"
    # ]
    stock_codes = None
    find_other_similar_trends(target_stock_code, start_date, end_date, stock_codes, data_dir, method="weighted",
                              trend_end_date=trend_end_date, same_market=True)


def fetch_ths_fupan():
    start_date = "20250830"
    # end_date = '20250512'
    end_date = None
    # all_fupan(start_date, end_date)
    all_fupan(start_date, end_date, types='all,else')


def draw_ths_fupan():
    start_date = '20250830'  # å¼€å§‹æ—¥æœŸ
    # end_date = '20250115'  # ç»“æŸæ—¥æœŸ
    end_date = None
    draw_fupan_lb(start_date, end_date)


def fupan_statistics_to_excel():
    # æŒ‡å®šæ—¶æ®µçš„å¤ç›˜æ€»ä½“å¤ç›˜æ•°æ®
    start_date = '20250810'
    # end_date = '20250228'
    end_date = None
    # åœ¨daily_routineä¸­å¼ºåˆ¶ä½¿ç”¨å•çº¿ç¨‹ï¼Œé¿å…å¤šçº¿ç¨‹å†²çª
    fupan_all_statistics(start_date, end_date, max_workers=1)


def fupan_statistics_excel_plot():
    start_date = '20250810'
    end_date = None
    plot_all(start_date, end_date)
    # plot_all()


def stocks_time_sharing_price():
    start_date = "20250512"
    end_date = "20250515"

    # æ‰‹åŠ¨æŒ‡å®š
    # stock_codes = ["600610", "601086", "302132", "002190", "002809"]
    stock_codes = ["603535", "002640", "600794", "603967", "603569"]
    # analyze_stocks_time_sharing(stock_codes, start_date, end_date)
    # è¯»å–å¼‚åŠ¨æ–‡ä»¶
    analyze_abnormal_stocks_time_sharing(start_date, end_date)


def plot_stock_daily_prices():
    # æŒ‡å®šè‚¡ç¥¨ä»£ç åˆ—è¡¨å’Œæ—¥æœŸèŒƒå›´
    stock_codes = ["603399", "600036", "601318", "000001", "600000"]
    start_date = "20250430"
    end_date = "20250523"

    # ç”»å‡ºæ—¥å¯¹æ¯”å›¾
    plot_multiple_stocks(stock_codes, start_date, end_date, equal_spacing=True)


def analyze_advanced_on():
    start_date = '2025-05-06'
    end_date = None
    analyze_rate(start_date, end_date)


def daily_group_analyze():
    start_date = "20250506"
    end_date = None
    find_stocks_by_hot_themes(start_date, end_date, top_n=5, weight_factor=3)
    # highlight_repeated_stocks()


def dejavu_fupan_analyze():
    # ç¤ºä¾‹ç”¨æ³•
    start_date = "20250421"
    end_date = "20250516"

    # å¤„ç†è¿æ¿æ•°æ®
    process_dejavu_data(start_date, end_date)


def update_synonym_groups():
    """
    æ›´æ–°åŒä¹‰è¯åˆ†ç»„ï¼ŒåŸºäºå·²æœ‰çš„æ¶¨åœåŸå› æ•°æ®æ–‡ä»¶
    å¯ç”¨äºè‡ªåŠ¨æ›´æ–°theme_color_util.pyä¸­çš„synonym_groups
    """
    # åˆ›å»ºåŒä¹‰è¯åˆ†ç»„ç®¡ç†å™¨
    manager = SynonymManager(threshold=0.8, min_group_size=3)

    # è‡ªåŠ¨å¤„ç†åŒä¹‰è¯åˆ†ç»„æ›´æ–°
    manager.update_from_latest_file(debug_phrases=["ä¸€ä½“åŒ–å‹é“¸"])


def whimsical_fupan_analyze():
    # æ‰§è¡Œå½’ç±»åˆ†æ
    start_date = "20250720"
    end_date = None

    process_zt_data(start_date, end_date, clean_output=True)
    # add_vba_for_excel()

    # ä¸ºã€æœªåˆ†ç±»åŸå› ã€‘å½’ç±»1
    # consolidate_unclassified_reasons()


def generate_ladder_chart():
    start_date = '20250801'  # è°ƒæ•´ä¸ºExcelä¸­æœ‰æ•°æ®çš„æ—¥æœŸèŒƒå›´
    end_date = None  # è¿‡äº†0ç‚¹éœ€æŒ‡å®šæ—¥æœŸ
    min_board_level = 2
    non_main_board_level = 2
    show_period_change = True  # æ˜¯å¦è®¡ç®—å‘¨æœŸæ¶¨è·Œå¹…
    sheet_name = None

    # å®šä¹‰ä¼˜å…ˆåŸå› åˆ—è¡¨
    priority_reasons = [
        # "åˆ›æ–°è¯"
    ]

    # æ„å»ºæ¢¯é˜Ÿå›¾
    build_ladder_chart(start_date, end_date, min_board_level=min_board_level,
                       non_main_board_level=non_main_board_level, show_period_change=show_period_change,
                       priority_reasons=priority_reasons, enable_attention_criteria=True,
                       sheet_name=sheet_name, create_leader_sheet=True, create_volume_sheet=True)


def generate_comparison_charts(recent_days: int = 10):
    """
    ç”Ÿæˆè‚¡ç¥¨ä¿¡å·å¯¹æ¯”å›¾ - æ ¹æ®ä¿¡å·æ—¥æœŸåˆ†ç»„ï¼Œä¾¿äºå¯¹æ¯”æŸ¥çœ‹

    Args:
        recent_days: ç”Ÿæˆæœ€è¿‘å‡ å¤©çš„å¯¹æ¯”å›¾ï¼Œé»˜è®¤10å¤©
    """
    from bin.comparison_chart_generator import run_auto_generation
    
    return run_auto_generation(base_dir='bin/candidate_stocks_breakout', recent_days=recent_days)


def generate_rebound_comparison_charts(recent_days: int = 10):
    """
    ç”Ÿæˆæ­¢è·Œåå¼¹ç­–ç•¥çš„è‚¡ç¥¨ä¿¡å·å¯¹æ¯”å›¾

    Args:
        recent_days: ç”Ÿæˆæœ€è¿‘å‡ å¤©çš„å¯¹æ¯”å›¾ï¼Œé»˜è®¤10å¤©
    """
    from bin.comparison_chart_generator import run_auto_generation
    
    return run_auto_generation(base_dir='bin/candidate_stocks_rebound', recent_days=recent_days)


def auction_fengdan_analyze(date_str: str = None, show_plot: bool = False):
    """
    é›†åˆç«ä»·å°å•æ•°æ®å¤ç›˜åˆ†æ

    Args:
        date_str: æŒ‡å®šåˆ†ææ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼Œé»˜è®¤ä¸ºæœ€è¿‘äº¤æ˜“æ—¥
        show_plot: æ˜¯å¦æ˜¾ç¤ºå›¾è¡¨ï¼Œé»˜è®¤Falseï¼ˆé¿å…é˜»å¡ï¼‰
    """
    from analysis.auction_fengdan_analysis import AuctionFengdanAnalyzer

    analyzer = AuctionFengdanAnalyzer()
    result = analyzer.run_comprehensive_analysis(date_str=date_str, show_plot=show_plot)

    if result:
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“… åˆ†ææ—¥æœŸ: {result['date']}")
        print(f"ğŸ“Š æ¶¨åœ: {result['zt_count']} åªï¼Œè·Œåœ: {result['dt_count']} åªï¼Œç«ä»·å°æ¿: {result['auction_count']} åª")
        if result.get('report_file'):
            print(f"ğŸ“„ åˆ†ææŠ¥å‘Š: {result['report_file']}")
        if result.get('chart_file'):
            print(f"ğŸ“Š åˆ†æå›¾è¡¨: {result['chart_file']}")
    else:
        print("âŒ åˆ†æå¤±è´¥æˆ–æ— æ•°æ®")





if __name__ == '__main__':
    # === å¤ç›˜ç›¸å…³ ===
    # daily_routine()
    # backtrade_simulate()
    # pullback_rebound_simulate()  # æ­¢è·Œåå¼¹ç­–ç•¥å›æµ‹
    # run_psq_analysis()
    # find_candidate_stocks()
    # strategy_scan('a')
    # generate_comparison_charts()
    # pullback_rebound_scan('a')  # æ­¢è·Œåå¼¹ç­–ç•¥æ‰«æ
    generate_rebound_comparison_charts()
    # get_stock_datas()
    # fetch_ths_fupan()
    # draw_ths_fupan()
    # whimsical_fupan_analyze()
    # generate_ladder_chart()
    # update_synonym_groups()
    # fupan_statistics_to_excel()
    # fupan_statistics_excel_plot()
    # find_yidong()
    # daily_group_analyze()
    # analyze_advanced_on()
    # stocks_time_sharing_price()
    # plot_stock_daily_prices()
    # get_hot_clouds()
    # find_dragon()
    # find_similar_trends()
    # get_stock_concept_and_industry()
    # fetch_and_filter_top_yybph()
    # get_top_yyb_trades()
    # get_lhb_datas()
    # get_stock_minute_datas()
    # get_index_data()
    # check_stock_datas()

    # === å‚æ•°ä¼˜åŒ–åŠŸèƒ½ ===
    # 1. ç”Ÿæˆé…ç½®æ¨¡æ¿
    # generate_optimization_templates()
    # 2. è¿è¡Œå‚æ•°ä¼˜åŒ–ï¼ˆéœ€è¦å…ˆç”Ÿæˆå¹¶ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼‰
    # run_parameter_optimization("compare_config.yaml")

    # === é›†åˆç«ä»·å°å•æ•°æ®åŠŸèƒ½ ===
    # auction_fengdan_analyze()  # å¤ç›˜åˆ†æå°å•æ•°æ®
    # å®šæ—¶é‡‡é›†è¯·è¿è¡Œ: python alerting/auction_scheduler.py start
