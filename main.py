import logging
import os
import warnings

from bin import simulator
from bin.experiment_runner import run_comparison_experiment
from bin.resilience_scanner import run_filter
from bin.scanner_analyzer import scan_and_visualize_analyzer
from strategy.breakout_strategy import BreakoutStrategy
from strategy.breakout_strategy_v2 import BreakoutStrategyV2
from strategy.pullback_rebound_strategy import PullbackReboundStrategy
from strategy.scannable_pullback_rebound_strategy import ScannablePullbackReboundStrategy
from strategy.weekly_volume_momentum_strategy import WeeklyVolumeMomentumStrategy

from utils.logging_util import redirect_print_to_logger

# å¿½ç•¥jiebaåº“ä¸­çš„pkg_resourcesè­¦å‘Š
warnings.filterwarnings("ignore", category=DeprecationWarning)

from datetime import datetime
from analysis.calculate_limit_up_success_rate import analyze_rate
from analysis.daily_group import find_stocks_by_hot_themes
from analysis.dejavu import process_dejavu_data
from analysis.fupan_statistics import fupan_all_statistics
from analysis.fupan_statistics_plot import plot_all
from analysis.seek_historical_similar import find_other_similar_trends
from analysis.stock_price_plotter import plot_multiple_stocks
from analysis.time_price_sharing import analyze_abnormal_stocks_time_sharing
from analysis.whimsical import process_zt_data
from analysis.ladder_chart import build_ladder_chart
from analysis.erban_longtou_analyzer import analyze_erban_longtou
from fetch.astock_concept import fetch_and_save_stock_concept
from fetch.astock_data import StockDataFetcher
from fetch.astock_data_minutes import fetch_and_save_stock_data
from fetch.indexes_data import fetch_indexes_data
from fetch.lhb_data import fetch_and_merge_stock_lhb_detail, fetch_and_filter_yybph_lhb_data, fetch_yyb_lhb_data, \
    find_top_yyb_trades
from fetch.tonghuashun.fupan import all_fupan
from fetch.tonghuashun.fupan_plot import draw_fupan_lb
from fetch.tonghuashun.fupan_plot_html import draw_fupan_lb_html
from fetch.tonghuashun.hotpoint_analyze import hot_words_cloud

from filters.find_abnormal import find_serious_abnormal_stocks_range
from filters.find_longtou import find_dragon_stocks
from utils.synonym_manager import SynonymManager
from bin.psq_analyzer import run_psq_analysis_report
from bin.parameter_optimizer import ParameterOptimizer
from bin.batch_backtester import batch_backtest_from_file, batch_backtest_from_list
from bin.selection_history_tracker import record_from_directory
from utils.backtrade.selection_review_visualizer import review_historical_selections

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(threadName)s] %(levelname)s - %(message)s')


def run_psq_analysis():
    """
    ä¸€é”®åŒ–PSQç»¼åˆåˆ†ææŠ¥å‘Šçš„æ–°å…¥å£
    """
    run_psq_analysis_report()


def run_parameter_optimization(config_name=None):
    """
    è¿è¡Œå‚æ•°ä¼˜åŒ–

    Args:
        config_name: é…ç½®æ–‡ä»¶åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    """
    optimizer = ParameterOptimizer()

    if config_name is None:
        # ç”Ÿæˆé»˜è®¤é…ç½®æ¨¡æ¿
        print("æœªæŒ‡å®šé…ç½®æ–‡ä»¶ï¼Œç”Ÿæˆé»˜è®¤é…ç½®æ¨¡æ¿...")
        template_path = optimizer.generate_config_template("default")
        print(f"é»˜è®¤é…ç½®æ¨¡æ¿å·²ç”Ÿæˆ: {template_path}")
        print("è¯·ç¼–è¾‘é…ç½®æ–‡ä»¶åé‡æ–°è¿è¡Œ")
        return template_path
    else:
        # è¿è¡Œä¼˜åŒ–
        config_path = f"bin/optimization_configs/{config_name}"
        if not config_name.endswith('.yaml'):
            config_path += '.yaml'

        if not os.path.exists(config_path):
            print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return None

        print(f"å¼€å§‹è¿è¡Œå‚æ•°ä¼˜åŒ–ï¼Œé…ç½®æ–‡ä»¶: {config_path}")
        report_path = optimizer.run_optimization(config_path)
        print(f"å‚æ•°ä¼˜åŒ–å®Œæˆï¼æŠ¥å‘Šä¿å­˜åœ¨: {report_path}")
        return report_path


def generate_optimization_templates():
    """
    ç”Ÿæˆå„ç§ç±»å‹çš„ä¼˜åŒ–é…ç½®æ¨¡æ¿
    """
    optimizer = ParameterOptimizer()

    templates = {
        "default": "é»˜è®¤é…ç½®æ¨¡æ¿",
        "quick": "å¿«é€Ÿæµ‹è¯•æ¨¡æ¿",
        "grid": "ç½‘æ ¼æœç´¢æ¨¡æ¿",
        "compare": "å‚æ•°æ–‡ä»¶å¯¹æ¯”æ¨¡æ¿"
    }

    generated_files = []
    for template_type, description in templates.items():
        if template_type == "compare":
            # compareæ¨¡æ¿ä¸éœ€è¦é¢å¤–å‚æ•°
            template_path = optimizer.generate_config_template(template_type=template_type)
        else:
            template_path = optimizer.generate_config_template(template_type=template_type,
                                                               strategy_class=BreakoutStrategy,
                                                               test_params=['consolidation_lookback',
                                                                            'consolidation_ma_proximity_pct',
                                                                            'consolidation_ma_max_slope'])
        generated_files.append(template_path)
        print(f"{description}å·²ç”Ÿæˆ: {template_path}")

    print(f"\næ€»å…±ç”Ÿæˆäº† {len(generated_files)} ä¸ªé…ç½®æ¨¡æ¿")
    print("è¯·æ ¹æ®éœ€è¦ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œç„¶åè¿è¡Œå‚æ•°ä¼˜åŒ–")

    return generated_files


# å›æº¯äº¤æ˜“
def backtrade_simulate():
    # æ‰¹é‡å›æµ‹å¹¶å¯¹æ¯”
    # run_comparison_experiment()

    # å•ä¸ªå›æµ‹
    # stock_code = '300128'
    # stock_code = '301217'
    stock_code = '603232'
    simulator.go_trade(
        code=stock_code,
        amount=100000,
        startdate=datetime(2025, 1, 1),
        enddate=datetime(2025, 10, 31),
        strategy=BreakoutStrategy,
        strategy_params={'debug': True, 'enable_prior_high_score': True},  # å¼€å¯è¯¦ç»†æ—¥å¿—
        log_trades=True,
        visualize=True,
        interactive_plot=True,  # å¼¹å‡ºäº¤äº’å›¾
    )


# å¤§æ‰¹é‡å›æµ‹ï¼ˆä»æ–‡ä»¶è¯»å–è‚¡ç¥¨åˆ—è¡¨ï¼‰
def batch_backtest_from_stock_list():
    """
    å¤§æ‰¹é‡è‚¡ç¥¨å›æµ‹ - ä»æ–‡ä»¶è¯»å–è‚¡ç¥¨åˆ—è¡¨
    
    ç‰¹ç‚¹ï¼š
    1. æ”¯æŒä»CSV/TXTæ–‡ä»¶è¯»å–è‚¡ç¥¨åˆ—è¡¨
    2. å¤šè¿›ç¨‹å¹¶è¡Œå›æµ‹ï¼Œå¤§å¹…æå‡æ€§èƒ½
    3. åªè¾“å‡ºæ±‡æ€»ç»Ÿè®¡ï¼Œä¸ç”Ÿæˆè¯¦ç»†å›¾è¡¨
    4. ç”ŸæˆExcelæ±‡æ€»æŠ¥å‘Šï¼ŒåŒ…å«ç»Ÿè®¡åˆ†æ
    
    ä½¿ç”¨åœºæ™¯ï¼š
    - å…¨å¸‚åœºæ‰«æï¼ˆå¦‚5000åªAè‚¡ï¼‰
    - æ¿å—æ‰¹é‡å›æµ‹
    - ç­–ç•¥æ‰¹é‡éªŒè¯
    """
    # è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒCSVæˆ–TXTæ ¼å¼ï¼‰
    stock_list_file = 'data/batch_backtest/all_astocks.txt'  # å¯ä»¥æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„

    # å¦‚æœæ²¡æœ‰ç°æˆæ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ä»£ç åˆ—è¡¨ï¼ˆè§ä¸‹é¢çš„batch_backtest_from_codeså‡½æ•°ï¼‰

    report_path = batch_backtest_from_file(
        stock_list_file=stock_list_file,
        strategy_class=BreakoutStrategyV2,
        strategy_params={
            'debug': False,  # æ‰¹é‡å›æµ‹å»ºè®®å…³é—­è¯¦ç»†æ—¥å¿—
        },
        startdate=datetime(2022, 1, 1),
        enddate=datetime(2025, 10, 21),
        amount=100000,
        data_dir='./data/astocks',
        output_dir='bin/batch_backtest_results',
        max_workers=None,  # Noneè¡¨ç¤ºè‡ªåŠ¨ä½¿ç”¨CPUæ ¸å¿ƒæ•°-1ï¼Œä¹Ÿå¯æ‰‹åŠ¨æŒ‡å®šå¦‚4ã€8ç­‰
        resume=False  # æ˜¯å¦æ–­ç‚¹ç»­ä¼ ï¼ˆè·³è¿‡å·²å®Œæˆçš„è‚¡ç¥¨ï¼‰
    )

    print(f"\næ‰¹é‡å›æµ‹å®Œæˆï¼æŠ¥å‘Šè·¯å¾„: {report_path}")


# å¤§æ‰¹é‡å›æµ‹ï¼ˆç›´æ¥ä½¿ç”¨ä»£ç åˆ—è¡¨ï¼‰
def batch_backtest_from_codes():
    """
    å¤§æ‰¹é‡è‚¡ç¥¨å›æµ‹ - ç›´æ¥æä¾›è‚¡ç¥¨ä»£ç åˆ—è¡¨
    
    é€‚åˆä»£ç ä¸å¤šçš„åœºæ™¯ï¼Œæˆ–è€…åŠ¨æ€ç”Ÿæˆè‚¡ç¥¨æ± çš„åœºæ™¯
    """
    # æ–¹å¼1: æ‰‹åŠ¨æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨
    stock_codes = ['300033', '300059', '000062', '300204', '600610', '002693', '301357', '600744', '002173', '002640',
                   '002104', '002658']

    # æ–¹å¼2: ä»å…¶ä»–æ¥æºè·å–ï¼ˆç¤ºä¾‹ï¼šè¯»å–æŸä¸ªæ¿å—çš„æ‰€æœ‰è‚¡ç¥¨ï¼‰
    # from fetch.astock_concept import get_concept_stocks
    # stock_codes = get_concept_stocks('æ–°èƒ½æºè½¦')

    report_path = batch_backtest_from_list(
        stock_codes=stock_codes,
        strategy_class=BreakoutStrategyV2,
        strategy_params={'debug': False},
        startdate=datetime(2022, 1, 1),
        enddate=datetime(2025, 10, 27),
        amount=100000,
        data_dir='./data/astocks',
        output_dir='bin/batch_backtest_results',
        max_workers=4  # å¯æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
    )

    print(f"\næ‰¹é‡å›æµ‹å®Œæˆï¼æŠ¥å‘Šè·¯å¾„: {report_path}")


# ç”Ÿæˆæ‰¹é‡å›æµ‹ç”¨çš„è‚¡ç¥¨åˆ—è¡¨
def generate_stock_lists():
    """
    ä»æ•°æ®ç›®å½•ç”Ÿæˆè‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶ï¼Œä¸ºæ‰¹é‡å›æµ‹åšå‡†å¤‡
    
    åŠŸèƒ½ï¼š
    - ç”Ÿæˆå…¨éƒ¨Aè‚¡åˆ—è¡¨
    - æŒ‰å¸‚åœºåˆ†ç±»ï¼ˆæ²ªå¸‚ã€æ·±å¸‚ã€åŒ—äº¤æ‰€ï¼‰
    - æŒ‰æ¿å—åˆ†ç±»ï¼ˆä¸»æ¿ã€åˆ›ä¸šæ¿ã€ç§‘åˆ›æ¿ï¼‰
    
    è¾“å‡ºç›®å½•ï¼šdata/batch_backtest/
    """
    from bin.generate_stock_list import generate_all
    generate_all()


# ä»å¤ç›˜æ•°æ®ç”Ÿæˆæ‰¹é‡å›æµ‹å€™é€‰è‚¡
def generate_fupan_candidates():
    """
    ä»å¤ç›˜æ•°æ®æ–‡ä»¶æå–çƒ­é—¨è‚¡ä½œä¸ºæ‰¹é‡å›æµ‹å€™é€‰
    
    åŠŸèƒ½ï¼š
    - ä» excel/fupan_stocks.xlsx æå–æŒ‡å®šç±»å‹çš„è‚¡ç¥¨
    - æ”¯æŒæŒ‰æ—¥æœŸèŒƒå›´ç­›é€‰
    - æ”¯æŒå¤šä¸ªsheetç»„åˆï¼ˆå¦‚è¿æ¿+é¦–æ¿+å¤§æ¶¨ï¼‰
    
    ä½¿ç”¨åœºæ™¯ï¼š
    - å›æµ‹ç‰¹å®šæ—¶æœŸçš„çƒ­é—¨è‚¡è¡¨ç°
    - éªŒè¯ç­–ç•¥åœ¨å¼ºåŠ¿è‚¡ä¸Šçš„æ•ˆæœ
    - ç¼©å°å›æµ‹èŒƒå›´æé«˜æ•ˆç‡
    """
    from bin.generate_stock_list import generate_fupan_stock_list

    # ç¤ºä¾‹1: æå–å¤šç§ç±»å‹çš„çƒ­é—¨è‚¡
    generate_fupan_stock_list(
        sheet_names=['è¿æ¿æ•°æ®', 'é»˜é»˜ä¸Šæ¶¨', 'å…³æ³¨åº¦æ¦œ', 'éä¸»å…³æ³¨åº¦æ¦œ'],
        start_date='20250901',
        end_date='20251020',
        output_prefix='hot_stocks_202509'
    )

    # ç¤ºä¾‹2: æå–æ‰€æœ‰ç±»å‹çš„è‚¡ç¥¨ï¼ˆä¸é™æ—¥æœŸï¼‰
    # generate_fupan_stock_list(
    #     sheet_names=None,  # Noneè¡¨ç¤ºæ‰€æœ‰sheet
    #     start_date=None,   # Noneè¡¨ç¤ºä»æœ€æ—©å¼€å§‹
    #     end_date=None,     # Noneè¡¨ç¤ºåˆ°æœ€æ™š
    #     output_prefix='all_fupan_stocks'
    # )


# æ­¢è·Œåå¼¹ç­–ç•¥å›æµ‹
def pullback_rebound_simulate():
    """
    æ­¢è·Œåå¼¹ç­–ç•¥å›æµ‹ç¤ºä¾‹
    
    ç­–ç•¥è¯´æ˜ï¼š
    - è¯†åˆ«ä¸»å‡æµªåçš„å›è°ƒä¼ç¨³æœºä¼š
    - é€šè¿‡é‡ä»·èƒŒç¦»ã€é‡çª’æ¯ã€ä¼ç¨³Kçº¿ä¸‰ä¸ªä¿¡å·åˆ¤æ–­åå¼¹æ—¶æœº
    - æ­¢ç›ˆ12%ã€æ­¢æŸ5%ã€æœ€å¤§æŒæœ‰10å¤©
    """
    stock_code = '002554'  # å¯ä»¥æ›¿æ¢ä¸ºå…¶ä»–è‚¡ç¥¨ä»£ç 
    simulator.go_trade(
        code=stock_code,
        amount=100000,
        startdate=datetime(2025, 9, 1),
        enddate=datetime(2025, 10, 31),
        strategy=PullbackReboundStrategy,
        strategy_params={
            # -- è°ƒè¯•å‚æ•° --
            'debug': True,  # æ˜¯å¦å¼€å¯è¯¦ç»†æ—¥å¿—

            # -- ä¸»å‡æµªè¯†åˆ«å‚æ•°ï¼ˆå¯è°ƒæ•´ï¼‰--
            'uptrend_min_gain': 0.30,  # ä¸»å‡æµªæœ€å°æ¶¨å¹…30%ï¼Œè¶Šå¤§è¶Šä¸¥æ ¼
            'volume_surge_ratio': 1.5,  # ä¸»å‡æµªæ”¾é‡å€æ•°ï¼Œè¶Šå¤§è¦æ±‚è¶Šé«˜

            # -- å›è°ƒè¯†åˆ«å‚æ•°ï¼ˆå¯è°ƒæ•´ï¼‰--
            # 'pullback_max_ratio': 0.5,  # æœ€å¤§å›è°ƒå¹…åº¦50%
            # 'pullback_max_days': 15,    # æœ€å¤§å›è°ƒå¤©æ•°15å¤©

            # -- äº¤æ˜“å‚æ•°ï¼ˆå¯è°ƒæ•´ï¼‰--
            # 'initial_stake_pct': 0.8,   # åˆå§‹ä»“ä½80%
            # 'profit_target': 0.12,      # æ­¢ç›ˆç›®æ ‡12%
            # 'stop_loss': 0.05,          # æ­¢æŸæ¯”ä¾‹5%
            # 'max_hold_days': 10,        # æœ€å¤§æŒæœ‰å¤©æ•°10å¤©
        },
        log_trades=True,
        visualize=True,
        interactive_plot=True,  # å¼¹å‡ºäº¤äº’å›¾
    )


# å‘¨é‡èƒ½æ”¾å¤§ç­–ç•¥å›æµ‹
def weekly_volume_momentum_simulate():
    """å‘¨é‡èƒ½æ”¾å¤§ + çŸ­æœŸæ¸©å’Œä¸Šè¡Œ ç­–ç•¥å›æµ‹ç¤ºä¾‹"""
    stock_code = '000009'  # å¯æ›¿æ¢æ ‡çš„
    simulator.go_trade(
        code=stock_code,
        amount=100000,
        startdate=datetime(2021, 1, 1),
        enddate=datetime(2025, 10, 13),
        strategy=WeeklyVolumeMomentumStrategy,
        strategy_params={
            'debug': True,
            'week_window': 5,
            'week_growth_ratio': 2.0,
            'three_month_lookback_days': 60,
            'three_month_max_change': 0.401,
            'gap_open_exclude_pct': 0.05,
            'daily_gain_upper_pct': 0.045,
            'initial_stake_pct': 1.0,
            'base_hold_days': 2,
            'stop_loss_pct': 0.05,
            'trigger_trailing_profit_pct': 0.09,
            'trailing_drawdown_pct': 0.01,
        },
        log_trades=True,
        visualize=True,
        interactive_plot=True,
    )


def strategy_scan(candidate_model='a'):
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„ä¿¡å·æ¨¡å¼åˆ—è¡¨
    signal_patterns = [
        # '*** è§¦å‘ã€çªç ´è§‚å¯Ÿå“¨ã€‘',
        # 'çªç ´ä¿¡å·',
        '*** äºŒæ¬¡ç¡®è®¤ä¿¡å·',  # æ ‡å‡†é€šé“ï¼šè§‚å¯ŸæœŸå†…äºŒæ¬¡ç¡®è®¤
        'ä¹°å…¥ä¿¡å·: å¿«é€Ÿé€šé“',  # å¿«é€Ÿé€šé“ï¼šä¿¡å·æ—¥å½“å¤©ä¹°å…¥
        'ä¹°å…¥ä¿¡å·: å›è¸©ç¡®è®¤',  # ç¼“å†²é€šé“ï¼šå›è°ƒåä¹°å…¥
        'ä¹°å…¥ä¿¡å·: æ­¢æŸçº é”™',  # æ­¢æŸçº é”™ï¼šä»·æ ¼åˆé€‚ä¹°å…¥
    ]

    start_date = '20250910'
    end_date = None
    stock_pool = ['300581', '600475']
    details_after_date = '20251120'  # åªçœ‹è¿™ä¸ªæ—¥æœŸä¹‹åçš„

    # æ‰«æä¸å¯è§†åŒ–
    scan_and_visualize_analyzer(
        scan_strategy=BreakoutStrategy,
        scan_start_date=start_date,
        scan_end_date=end_date,
        stock_pool=None,
        signal_patterns=signal_patterns,
        details_after_date=details_after_date,  # åªæœ‰æ­¤æ—¥æœŸåä¿¡å·æ‰è¾“å‡ºè¯¦æƒ…
        candidate_model=candidate_model,
        output_path=f'bin/candidate_stocks_breakout_{candidate_model}'  # æŒ‡å®šè¾“å‡ºç›®å½•ï¼ŒæŒ‰æ¨¡å‹åŒºåˆ†
    )


def pullback_rebound_scan(candidate_model='a'):
    """æ­¢è·Œåå¼¹ç­–ç•¥æ‰«æ"""
    # ä½¿ç”¨æ­¢è·Œåå¼¹ç­–ç•¥çš„ä¿¡å·æ¨¡å¼
    signal_patterns = [
        '*** æ­¢è·Œåå¼¹ä¹°å…¥ä¿¡å·è§¦å‘',
        'æ­¢è·Œåå¼¹ä¿¡å·',
    ]

    start_date = '20250910'
    end_date = None
    details_after_date = '20251015'  # åªçœ‹è¿™ä¸ªæ—¥æœŸä¹‹åçš„

    # æ‰«æä¸å¯è§†åŒ–
    scan_and_visualize_analyzer(
        scan_strategy=ScannablePullbackReboundStrategy,
        scan_start_date=start_date,
        scan_end_date=end_date,
        stock_pool=None,
        signal_patterns=signal_patterns,
        details_after_date=details_after_date,  # åªæœ‰æ­¤æ—¥æœŸåä¿¡å·æ‰è¾“å‡ºè¯¦æƒ…
        candidate_model=candidate_model,
        output_path=f'bin/candidate_stocks_rebound_{candidate_model}'  # æŒ‡å®šè¾“å‡ºç›®å½•ï¼ŒæŒ‰æ¨¡å‹åŒºåˆ†
    )


def find_candidate_stocks():
    run_filter()


def find_candidate_stocks_weekly_growth(offset_days: int = 0):
    """
    å‘¨æˆäº¤é‡å¢é•¿ç­–ç•¥é€‰è‚¡
    
    Args:
        offset_days: æ—¶é—´åç§»é‡ï¼ˆå¤©æ•°ï¼‰ï¼Œé»˜è®¤0
            - 0: ä»¥Tæ—¥ä¸ºåŸºå‡†ï¼ˆä»Šå¤©ï¼‰
            - 1: ä»¥T-1æ—¥ä¸ºåŸºå‡†ï¼ˆæ˜¨å¤©ï¼‰
            - N: ä»¥T-Næ—¥ä¸ºåŸºå‡†
    """
    # ä½¿ç”¨æ–°çš„"å‘¨é‡å¢+å½“æ—¥æ¡ä»¶"ç­›é€‰å™¨
    from bin.weekly_growth_scanner import run_filter as run_weekly_filter
    run_weekly_filter(offset_days=offset_days)


def record_scan_to_history(base_dir: str, model: str):
    """
    è®°å½•æ‰«æç»“æœåˆ°å†å²æ–‡ä»¶
    
    Args:
        base_dir: æ‰«æç»“æœç›®å½•
        model: æ¨¡å¼æ ‡è¯† (å¦‚ 'breakout_a', 'rebound_a', 'breakout_b')
    """
    try:
        record_from_directory(base_dir, model)
        logging.info(f"å·²è®°å½• {model} æ¨¡å¼çš„æ‰«æç»“æœåˆ°å†å²æ–‡ä»¶")
    except Exception as e:
        logging.error(f"è®°å½•æ‰«æç»“æœåˆ°å†å²æ–‡ä»¶å¤±è´¥: {e}")


def review_history(start_date: str, end_date: str, model: str = None, before_days: int = 90):
    """
    å›é¡¾å†å²å€™é€‰è‚¡çš„åç»­èµ°åŠ¿
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸ (ä¿¡å·æ—¥æœŸ)ï¼Œæ ¼å¼ 'YYYY-MM-DD' æˆ– 'YYYYMMDD'
        end_date: ç»“æŸæ—¥æœŸ (ä¿¡å·æ—¥æœŸ)ï¼Œæ ¼å¼ 'YYYY-MM-DD' æˆ– 'YYYYMMDD'
        model: æ¨¡å¼ç­›é€‰ï¼Œå¦‚ 'rebound_a', 'breakout_a', 'breakout_b'ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
        before_days: ä¿¡å·æ—¥æœŸä¹‹å‰æ˜¾ç¤ºçš„å¤©æ•°ï¼ˆé»˜è®¤90å¤©ï¼‰
    
    Returns:
        ç”Ÿæˆçš„å¯¹æ¯”å›¾æ–‡ä»¶åˆ—è¡¨
    
    ç¤ºä¾‹:
        # å›é¡¾10æœˆ20æ—¥åˆ°10æœˆ24æ—¥æ‰€æœ‰æ¨¡å¼çš„å€™é€‰è‚¡
        review_history('2025-10-20', '2025-10-24')
        
        # åªå›é¡¾æ­¢è·Œåå¼¹ç­–ç•¥açš„å€™é€‰è‚¡
        review_history('2025-10-20', '2025-10-24', model='rebound_a')
    """
    try:
        files = review_historical_selections(start_date, end_date, model, before_days)

        if files:
            print(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(files)} å¼ å›é¡¾å¯¹æ¯”å›¾")
            print(f"ğŸ“ å›é¡¾å›¾ä¿å­˜åœ¨: bin/candidate_history/review_charts/")
            print("\nç”Ÿæˆçš„å›é¡¾å›¾:")
            for file in files:
                print(f"  ğŸ“Š {os.path.basename(file)}")
        else:
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å†å²è®°å½•æˆ–ç”Ÿæˆå¤±è´¥")

        return files

    except Exception as e:
        logging.error(f"å›é¡¾å†å²è®°å½•å¤±è´¥: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return []


def analyze_weekly_growth_win_rate(scan_file: str = None, high_ratio: float = 0.25, close_ratio: float = 0.75):
    """
    åˆ†æå‘¨æˆäº¤é‡å¢é•¿ç­–ç•¥çš„èƒœç‡
    
    Args:
        scan_file: æ‰«ææ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤åˆ†ææœ€æ–°æ–‡ä»¶ï¼‰
        high_ratio: T+2æ—¥é«˜ç‚¹å–å‡ºæ¯”ä¾‹ï¼ˆé»˜è®¤0.25ï¼Œå³1/4ï¼‰
        close_ratio: T+2æ—¥æ”¶ç›˜å–å‡ºæ¯”ä¾‹ï¼ˆé»˜è®¤0.75ï¼Œå³3/4ï¼‰
    
    äº¤æ˜“é€»è¾‘:
        - Tæ—¥æ”¶ç›˜åæ‰«æ
        - T+1æ—¥å¼€ç›˜ä»·ä¹°å…¥
        - T+2æ—¥é«˜ç‚¹å–å‡º high_ratio ä»“ä½
        - T+2æ—¥æ”¶ç›˜å–å‡º close_ratio ä»“ä½
    """
    from bin.weekly_growth_win_rate_analyzer import analyze_latest_or_specified
    return analyze_latest_or_specified(scan_file, high_ratio, close_ratio)


def batch_analyze_weekly_growth_win_rate(directory: str = 'bin/candidate_temp',
                                         pattern: str = r'candidate_stocks_weekly_growth_\d{8}\.txt$',
                                         high_ratio: float = 0.25,
                                         close_ratio: float = 0.75):
    """
    æ‰¹é‡åˆ†æç›®å½•ä¸‹æ‰€æœ‰æ‰«ææ–‡ä»¶çš„èƒœç‡ï¼Œç”Ÿæˆå•ä¸€æ±‡æ€»æŠ¥å‘Š
    
    Args:
        directory: æ‰«ææ–‡ä»¶ç›®å½•ï¼ˆé»˜è®¤bin/candidate_tempï¼‰
        pattern: æ–‡ä»¶åæ­£åˆ™åŒ¹é…æ¨¡å¼ï¼ˆé»˜è®¤åªåŒ¹é…weekly_growthæ ¼å¼ï¼‰
        high_ratio: T+2æ—¥é«˜ç‚¹å–å‡ºæ¯”ä¾‹ï¼ˆé»˜è®¤0.25ï¼‰
        close_ratio: T+2æ—¥æ”¶ç›˜å–å‡ºæ¯”ä¾‹ï¼ˆé»˜è®¤0.75ï¼‰
    
    Returns:
        str: ç”Ÿæˆçš„æ‰¹é‡æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    
    æŠ¥å‘Šå†…å®¹:
        - æ•´ä½“æ±‡æ€»ç»Ÿè®¡
        - å„æ—¥æœŸç»Ÿè®¡æ•°æ®
        - é™„å½•ï¼šæ¯ä¸ªæ—¥æœŸçš„ç›ˆåˆ©TOP3å’ŒäºæŸTOP3
    """
    from bin.weekly_growth_win_rate_analyzer import batch_analyze_with_pattern
    return batch_analyze_with_pattern(directory, pattern, high_ratio, close_ratio)


# è·å–çƒ­ç‚¹æ¦‚å¿µè¯äº‘
def get_hot_clouds(date: str = None, concept_only: bool = True):
    """
    ç”Ÿæˆæ¯æ—¥Aè‚¡çƒ­é—¨è‚¡çš„æ¦‚å¿µè¯äº‘å›¾
    
    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYYMMDD'ï¼Œé»˜è®¤ä¸º None è¡¨ç¤ºæœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥
        concept_only: æ˜¯å¦ä»…ç”Ÿæˆæ¦‚å¿µè¯äº‘å›¾ï¼Œé»˜è®¤ä¸º Trueã€‚ä¸º False æ—¶ç”Ÿæˆæ¦‚å¿µ+è¡Œä¸šåˆå¹¶å›¾
    """
    hot_words_cloud(date, concept_only)


def get_index_data():
    # æŒ‡å®šä¿å­˜ç›®å½•
    save_directory = "data/indexes"
    fetch_indexes_data(save_directory)


def execute_routine(steps, routine_name="è‡ªå®šä¹‰æµç¨‹"):
    """
    é€šç”¨çš„æµç¨‹æ‰§è¡Œå™¨

    Args:
        steps: æ­¥éª¤åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (function, description) æˆ– function
        routine_name: æµç¨‹åç§°ï¼Œç”¨äºæ—¥å¿—æ–‡ä»¶å‘½å
    """
    import time
    import threading
    import matplotlib

    # è®¾ç½®matplotlibä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé¿å…Tkinterçº¿ç¨‹é—®é¢˜
    matplotlib.use('Agg')

    # è·å–å½“å‰æ—¥æœŸç”¨äºæ—¥å¿—æ–‡ä»¶å‘½å
    current_date = datetime.now().strftime('%Y%m%d')
    routine_name_safe = routine_name.replace(" ", "_")
    log_filename = f"logs/{routine_name_safe}_{current_date}_{datetime.now().strftime('%H%M%S')}.log"

    # ç¡®ä¿logsç›®å½•å­˜åœ¨
    os.makedirs("logs", exist_ok=True)

    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_filename, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter('%(asctime)s - [%(threadName)s] %(levelname)s - %(message)s')
    file_handler.setFormatter(file_formatter)

    # è·å–æ ¹æ—¥å¿—è®°å½•å™¨å¹¶æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
    root_logger = logging.getLogger()
    root_logger.addHandler(file_handler)

    # åˆ›å»ºprintè¾“å‡ºé‡å®šå‘çš„logger
    # è®¾ç½®propagate=Falseé˜²æ­¢æ—¥å¿—å‘ä¸Šä¼ æ’­åˆ°root loggerï¼Œé¿å…é‡å¤è®°å½•
    print_logger = logging.getLogger('print_capture')
    print_logger.propagate = False
    print_logger.addHandler(file_handler)

    # åŒæ—¶åœ¨æ§åˆ¶å°æ˜¾ç¤ºç®€åŒ–ä¿¡æ¯
    print(f"=== å¼€å§‹{routine_name} {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===")
    print(f"è¯¦ç»†æ—¥å¿—ä¿å­˜åˆ°: {log_filename}")
    print(f"æ€»å…± {len(steps)} ä¸ªæ­¥éª¤")

    start_time = time.time()

    try:
        for i, step in enumerate(steps, 1):
            # è§£ææ­¥éª¤é…ç½®
            if isinstance(step, tuple):
                func, description = step
            else:
                func = step
                description = func.__name__

            step_start_time = time.time()

            print(f"\n[æ­¥éª¤{i}/{len(steps)}] å¼€å§‹{description}...")
            logging.info(f"=== æ­¥éª¤{i}: å¼€å§‹{description} ===")
            logging.info(f"å½“å‰ä¸»çº¿ç¨‹: {threading.current_thread().name}")
            logging.info(f"å½“å‰æ´»è·ƒçº¿ç¨‹æ•°: {threading.active_count()}")

            # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨é‡å®šå‘printåˆ°æ—¥å¿—æ–‡ä»¶ï¼ˆåªåœ¨æ­¥éª¤æ‰§è¡ŒæœŸé—´ï¼‰
            with redirect_print_to_logger(print_logger):
                # æ‰§è¡Œæ­¥éª¤
                func()

            step_duration = time.time() - step_start_time
            logging.info(f"=== æ­¥éª¤{i}: {description}å®Œæˆ (è€—æ—¶: {step_duration:.2f}ç§’) ===")
            logging.info(f"æ­¥éª¤å®Œæˆåæ´»è·ƒçº¿ç¨‹æ•°: {threading.active_count()}")
            print(f"âœ“ {description}å®Œæˆ (è€—æ—¶: {step_duration:.2f}ç§’)")

        total_duration = time.time() - start_time
        print(
            f"\n=== æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆï¼æ€»è€—æ—¶: {total_duration:.2f}ç§’ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===")
        logging.info(f"=== {routine_name}å…¨éƒ¨å®Œæˆ (æ€»è€—æ—¶: {total_duration:.2f}ç§’) ===")

    except Exception as e:
        error_msg = f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(f"\nâŒ {error_msg}")
        logging.error(error_msg, exc_info=True)
        raise
    finally:
        # ç§»é™¤æ–‡ä»¶å¤„ç†å™¨ï¼Œé¿å…é‡å¤æ·»åŠ 
        root_logger.removeHandler(file_handler)
        print_logger.removeHandler(file_handler)
        file_handler.close()
        print(f"\nè¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_filename}")


def daily_routine():
    """
    æ—¥å¸¸é‡åŒ–äº¤æ˜“æ•°æ®å¤„ç†æµç¨‹
    """
    # å®šä¹‰æ—¥å¸¸æµç¨‹æ­¥éª¤
    daily_steps = [
        # (get_stock_datas, "æ‹‰å–Aè‚¡äº¤æ˜“æ•°æ®"),
        (get_index_data, "æ‹‰å–å„å¤§æŒ‡æ•°æ•°æ®"),
        (fetch_ths_fupan, "æ‹‰å–çƒ­é—¨ä¸ªè‚¡æ•°æ®"),
        (whimsical_fupan_analyze, "æ‰§è¡Œé¢˜æåˆ†æ"),
        (generate_ladder_chart, "ç”Ÿæˆçƒ­é—¨è‚¡å¤©æ¢¯"),
        (draw_ths_fupan, "ç»˜åˆ¶æ¶¨è·Œé«˜åº¦å›¾"),
        (draw_ths_fupan_html, "ç”Ÿæˆæ¶¨è·Œé«˜åº¦html"),
        (fupan_statistics_to_excel, "ç”Ÿæˆç»Ÿè®¡æ•°æ®"),
        (fupan_statistics_excel_plot, "ç”Ÿæˆç»Ÿè®¡å›¾è¡¨"),
        (get_hot_clouds, "ç”Ÿæˆçƒ­é—¨æ¦‚å¿µè¯äº‘"),
        (auction_fengdan_analyze, "å¤ç›˜åˆ†æå°å•æ•°æ®"),
    ]

    execute_routine(daily_steps, "daily_routine")


def full_scan_routine(candidate_model='a'):
    """
    ä¸€é”®æ‰§è¡Œå®Œæ•´çš„ç­–ç•¥æ‰«æå’Œå¯¹æ¯”å›¾ç”Ÿæˆæµç¨‹
    """
    scan_steps = [
        (lambda: strategy_scan(candidate_model), "æ‰§è¡Œçªç ´ç­–ç•¥æ‰«æ"),
        (lambda: generate_comparison_charts(candidate_model), "ç”Ÿæˆçªç ´ç­–ç•¥å¯¹æ¯”å›¾"),
        (lambda: record_scan_to_history(f'bin/candidate_stocks_breakout_{candidate_model}',
                                        f'breakout_{candidate_model}'),
         f"è®°å½•çªç ´ç­–ç•¥{candidate_model}æ‰«æç»“æœ"),
        # (lambda: pullback_rebound_scan(candidate_model), "æ‰§è¡Œæ­¢è·Œåå¼¹ç­–ç•¥æ‰«æ"),
        # (lambda: generate_rebound_comparison_charts(candidate_model), "ç”Ÿæˆæ­¢è·Œåå¼¹ç­–ç•¥å¯¹æ¯”å›¾"),
        # (lambda: record_scan_to_history(f'bin/candidate_stocks_rebound_{candidate_model}', f'rebound_{candidate_model}'),
        #  f"è®°å½•æ­¢è·Œåå¼¹ç­–ç•¥{candidate_model}æ‰«æç»“æœ"),
        # (lambda: find_candidate_stocks_weekly_growth(), "ç­›é€‰å‘¨å¢é•¿çš„å€™é€‰è‚¡"),
        # (lambda: strategy_scan('b'), "æ‰§è¡Œçªç ´ç­–ç•¥æ‰«æb"),
        # (lambda: generate_comparison_charts('b'), "ç”Ÿæˆçªç ´ç­–ç•¥å¯¹æ¯”å›¾b"),
        # (lambda: record_scan_to_history('bin/candidate_stocks_breakout_b', 'breakout_b'),
        #  "è®°å½•çªç ´ç­–ç•¥bæ‰«æç»“æœ"),
    ]

    execute_routine(scan_steps, "full_scan_routine")


# æ‹‰aè‚¡å†å²æ•°æ®
def get_stock_datas():
    stock_list = ["600610", "300033"]
    use_realtime = True

    # åˆ›å»ºAè‚¡æ•°æ®è·å–å¯¹è±¡ï¼ŒæŒ‡å®šæ‹‰å–çš„å¤©æ•°å’Œä¿å­˜è·¯å¾„
    data_fetcher = StockDataFetcher(start_date='20250930', end_date=None, save_path='./data/astocks',
                                    max_workers=8, stock_list=None, force_update=False, max_sleep_time=2000)

    # æ ¹æ®å‚æ•°é€‰æ‹©ä¸åŒçš„æ•°æ®è·å–æ–¹å¼
    if use_realtime:
        # ä½¿ç”¨å®æ—¶æ•°æ®æ¥å£æ›´æ–°å½“å¤©æ•°æ®
        data_fetcher.fetch_and_save_data_from_realtime()
    else:
        # ä½¿ç”¨å†å²æ•°æ®æ¥å£è·å–æ•°æ®ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        data_fetcher.fetch_and_save_data()


def get_stock_minute_datas():
    fetch_and_save_stock_data(
        interval="15",  # æ‹‰å– 15 åˆ†é’Ÿçº§åˆ«æ•°æ®
        start_date="20241110",  # èµ·å§‹æ—¥æœŸ
        end_date="20241210",  # ç»ˆæ­¢æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        # stock_list=["000717", "603776"],  # åªæ‹‰å–è¿™ä¸¤ä¸ªè‚¡ç¥¨çš„æ•°æ®
        output_dir="./data/astocks_minute"  # ä¿å­˜åˆ°æŒ‡å®šç›®å½•
    )


def fetch_and_filter_top_yybph():
    # ä½¿ç”¨ç¤ºä¾‹
    symbol = "è¿‘ä¸‰æœˆ"
    file_path = "./data/lhb/top_yybph.csv"  # ä¿å­˜çš„ CSV æ–‡ä»¶è·¯å¾„ï¼Œè¯·æ ¹æ®éœ€è¦ä¿®æ”¹

    fetch_and_filter_yybph_lhb_data(symbol, file_path)


def get_lhb_datas():
    start_date = "2024-08-01"
    end_date = None
    first_file_path = './data/lhb/yyb_lhb_data.csv'
    second_file_path = './data/lhb/stock_lhb_details.csv'
    trader_name = "ä¸­å›½é“¶æ²³è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸å¤§è¿é»„æ²³è·¯è¯åˆ¸è¥ä¸šéƒ¨"
    # 1. æ‹‰å–è¥ä¸šéƒ¨é¾™è™æ¦œæ•°æ®
    fetch_yyb_lhb_data(start_date, end_date, first_file_path)
    # 2. éå†è¥ä¸šéƒ¨æ•°æ®ï¼Œæ‹‰å–ä¸ªè‚¡é¾™è™æ¦œåˆå¹¶
    fetch_and_merge_stock_lhb_detail(first_file_path, second_file_path, trader_name)


def get_top_yyb_trades():
    # æ‰¾æœ€é¡¶çº§æ¸¸èµ„çš„äº¤æ˜“æ•°æ®
    # éœ€å…ˆè¿è¡Œfetch_and_filter_top_yybph()è·å–"top_yybph_lhb_data.csv"
    find_top_yyb_trades('./data/lhb/')


# æ‰¾é¾™å¤´
def find_dragon():
    start_date = '2025-01-01'
    # end_date = '2025-02-28'
    end_date = None
    find_dragon_stocks(start_date, end_date, threshold=180)


def find_yidong():
    # date = '2025-04-28'
    # find_serious_abnormal_stocks(date, check_updown_fluctuation=False)

    start_date = '2025-05-06'
    end_date = None
    find_serious_abnormal_stocks_range(start_date, end_date)


def get_stock_concept_and_industry():
    fetch_and_save_stock_concept(
        concept_list=["äº‘æ¸¸æˆ", "æ–°èƒ½æºè½¦"],
        industry_list=["é“¶è¡Œ", "æˆ¿åœ°äº§"],
        output_path="./excel/all_concepts.xlsx"
    )


def find_similar_trends():
    data_dir = "./data/astocks"  # æ•°æ®æ–‡ä»¶æ‰€åœ¨ç›®å½•
    target_stock_code = "300611"  # ç›®æ ‡è‚¡ç¥¨ä»£ç 
    start_date = '20250911'  # ç›®æ ‡è‚¡ç¥¨çš„èµ·å§‹æ—¥æœŸï¼ˆå­—ç¬¦ä¸²æ ¼å¼ YYYYMMDDï¼‰
    end_date = '20251023'  # ç›®æ ‡è‚¡ç¥¨çš„ç»“æŸæ—¥æœŸ

    # 1.å¯»æ‰¾è‡ªèº«ç›¸ä¼¼æ—¶æœŸ
    # target_index_code = "sz399001"  # ç›®æ ‡æŒ‡æ•°ä»£ç 
    # find_self_similar_windows(target_index_code, '20250815', '20251014', "./data/indexes", method="weighted")

    # 2.å¯»æ‰¾åŒæ—¶æœŸç›¸ä¼¼ä¸ªè‚¡
    # å¯é€‰è‚¡ç¥¨ä»£ç åˆ—è¡¨
    # stock_codes = [
    #     "600928",
    #     "601319",
    #     "001227"
    # ]
    stock_codes = None

    # ============= ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•è¯´æ˜ =============
    # methodå¯é€‰å€¼ï¼š
    # - "close_price": ä»…æ”¶ç›˜ä»·ç›¸å…³æ€§ï¼ˆæœ€å¿«ä½†æœ€ç²—ç³™ï¼‰
    # - "weighted": åŸåŠ æƒç›¸å…³æ€§ï¼ˆå¿«é€Ÿï¼Œé€‚åˆç²—ç•¥ç­›é€‰ï¼‰
    # - "enhanced_weighted": å¢å¼ºç‰ˆåŠ æƒç›¸å…³æ€§ï¼ˆæ¨èï¼è€ƒè™‘é‡ä»·é…åˆå’Œé˜¶æ®µæ€§ç‰¹å¾ï¼‰
    # - "dtw": DTWåŠ¨æ€æ—¶é—´è§„æ•´ï¼ˆæœ€ç²¾ç¡®ä½†æœ€æ…¢ï¼‰

    # ============= æ‰§è¡Œç›¸ä¼¼è¶‹åŠ¿æŸ¥æ‰¾ï¼ˆå…ˆé€‰æ‹©æ¨¡å¼ï¼‰ =============
    # æ¨¡å¼1: å•ä¸€ç»“æŸæ—¥æœŸï¼ˆé€‚åˆç¡®å®šæŸä¸ªæ—¶ç‚¹çš„ç›¸ä¼¼èµ°åŠ¿ï¼‰
    trend_end_date = '20251023'  # è¢«æŸ¥æ‰¾è‚¡ç¥¨çš„è¶‹åŠ¿ç»“æŸæ—¥æœŸ
    trend_date_range = None  # è®¾ä¸ºNoneè¡¨ç¤ºä½¿ç”¨å•ä¸€æ—¥æœŸæ¨¡å¼
    search_step = None  # å•ä¸€æ—¥æœŸæ— æ­¥é•¿

    # æ¨¡å¼2: æ—¶é—´æ®µæ‰«æï¼ˆé€‚åˆæŸ¥æ‰¾å†å²ä¸Šä»»æ„æ—¶æœŸçš„ç›¸ä¼¼èµ°åŠ¿ï¼‰â­æ¨è
    # trend_end_date = None
    # trend_date_range = ('20250716', '20251015')  # åœ¨è¿™ä¸ªæ—¶é—´æ®µå†…æ»‘åŠ¨çª—å£æŸ¥æ‰¾
    # search_step = 5  # âš ï¸ æ­¥é•¿è®¾ç½®å½±å“ç»“æœï¼Œå°æ­¥é•¿æå¤§å½±å“æ€§èƒ½ï¼šå¦‚æœå•ä¸€æ—¥æœŸèƒ½æ‰¾åˆ°é«˜ç›¸ä¼¼åº¦ï¼Œä½†æ—¶é—´æ®µæ‰¾ä¸åˆ°ï¼Œè¯´æ˜æ­¥é•¿å¤ªå¤§äº†ï¼

    find_other_similar_trends(
        target_stock_code, start_date, end_date, stock_codes, data_dir,
        method="weighted",  # ä½¿ç”¨å¢å¼ºç‰ˆæ–¹æ³•
        trend_end_date=trend_end_date,  # æ¨¡å¼1å‚æ•°
        trend_date_range=trend_date_range,  # æ¨¡å¼2å‚æ•°ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
        search_step=search_step,  # å¯é€‰ï¼šè‡ªå®šä¹‰æ­¥é•¿ï¼ˆä»…æ¨¡å¼2æœ‰æ•ˆï¼‰
        same_market=True
    )


def fetch_ths_fupan():
    start_date = "20250930"
    # end_date = '20250512'
    end_date = None
    # all_fupan(start_date, end_date)
    all_fupan(start_date, end_date, types='all,else')


def draw_ths_fupan():
    start_date = '20250930'  # å¼€å§‹æ—¥æœŸ
    # end_date = '20250115'  # ç»“æŸæ—¥æœŸ
    end_date = None
    draw_fupan_lb(start_date, end_date)


def draw_ths_fupan_html():
    """
    ç”ŸæˆHTMLäº¤äº’å¼å¤ç›˜å›¾
    """
    start_date = '20250930'  # å¼€å§‹æ—¥æœŸ
    # end_date = '20250115'  # ç»“æŸæ—¥æœŸ
    end_date = None
    draw_fupan_lb_html(start_date, end_date)


def fupan_statistics_to_excel():
    # æŒ‡å®šæ—¶æ®µçš„å¤ç›˜æ€»ä½“å¤ç›˜æ•°æ®
    start_date = '20250930'
    # end_date = '20250228'
    end_date = None
    # åœ¨daily_routineä¸­å¼ºåˆ¶ä½¿ç”¨å•çº¿ç¨‹ï¼Œé¿å…å¤šçº¿ç¨‹å†²çª
    fupan_all_statistics(start_date, end_date, max_workers=1)


def fupan_statistics_excel_plot():
    start_date = '20250930'
    end_date = None
    plot_all(start_date, end_date)
    # plot_all()


def stocks_time_sharing_price():
    start_date = "20250512"
    end_date = "20250515"

    # æ‰‹åŠ¨æŒ‡å®š
    # stock_codes = ["600610", "601086", "302132", "002190", "002809"]
    stock_codes = ["603535", "002640", "600794", "603967", "603569"]
    # analyze_stocks_time_sharing(stock_codes, start_date, end_date)
    # è¯»å–å¼‚åŠ¨æ–‡ä»¶
    analyze_abnormal_stocks_time_sharing(start_date, end_date)


def plot_stock_daily_prices():
    # æŒ‡å®šè‚¡ç¥¨ä»£ç åˆ—è¡¨å’Œæ—¥æœŸèŒƒå›´
    stock_codes = ["603399", "600036", "601318", "000001", "600000"]
    start_date = "20250430"
    end_date = "20250523"

    # ç”»å‡ºæ—¥å¯¹æ¯”å›¾
    plot_multiple_stocks(stock_codes, start_date, end_date, equal_spacing=True)


def analyze_advanced_on():
    start_date = '2025-05-06'
    end_date = None
    analyze_rate(start_date, end_date)


def daily_group_analyze():
    start_date = "20250506"
    end_date = None
    find_stocks_by_hot_themes(start_date, end_date, top_n=5, weight_factor=3)
    # highlight_repeated_stocks()


def dejavu_fupan_analyze():
    # ç¤ºä¾‹ç”¨æ³•
    start_date = "20250421"
    end_date = "20250516"

    # å¤„ç†è¿æ¿æ•°æ®
    process_dejavu_data(start_date, end_date)


def update_synonym_groups():
    """
    æ›´æ–°åŒä¹‰è¯åˆ†ç»„ï¼ŒåŸºäºå·²æœ‰çš„æ¶¨åœåŸå› æ•°æ®æ–‡ä»¶
    å¯ç”¨äºè‡ªåŠ¨æ›´æ–°theme_color_util.pyä¸­çš„synonym_groups
    """
    # åˆ›å»ºåŒä¹‰è¯åˆ†ç»„ç®¡ç†å™¨
    manager = SynonymManager(threshold=0.8, min_group_size=3)

    # è‡ªåŠ¨å¤„ç†åŒä¹‰è¯åˆ†ç»„æ›´æ–°
    manager.update_from_latest_file(debug_phrases=["ä¸€ä½“åŒ–å‹é“¸"])


def clean_synonym_groups(lookback_days=60, dry_run=False):
    """
    æ¸…ç†synonym_groupsä¸­æœªä½¿ç”¨çš„æ—§æ¦‚å¿µè¯
    """
    from utils.synonym_cleaner import SynonymCleaner

    cleaner = SynonymCleaner(lookback_days=lookback_days)
    cleaner.clean(dry_run=dry_run)


def whimsical_fupan_analyze():
    # æ‰§è¡Œå½’ç±»åˆ†æ
    start_date = "20250930"
    end_date = None

    process_zt_data(start_date, end_date, clean_output=True)
    # add_vba_for_excel()

    # ä¸ºã€æœªåˆ†ç±»åŸå› ã€‘å½’ç±»1
    # consolidate_unclassified_reasons()


def generate_ladder_chart():
    start_date = '20251020'  # è°ƒæ•´ä¸ºExcelä¸­æœ‰æ•°æ®çš„æ—¥æœŸèŒƒå›´
    end_date = None  # è¿‡äº†0ç‚¹éœ€æŒ‡å®šæ—¥æœŸ
    min_board_level = 2
    non_main_board_level = 2
    show_period_change = True  # æ˜¯å¦è®¡ç®—å‘¨æœŸæ¶¨è·Œå¹…
    sheet_name = None

    # å®šä¹‰ä¼˜å…ˆåŸå› åˆ—è¡¨
    priority_reasons = [
        # "æµ·å³¡ä¸¤å²¸",
    ]
    # å®šä¹‰ä½ä¼˜å…ˆåŸå› åˆ—è¡¨ï¼ˆåªæœ‰åœ¨æ²¡æœ‰å…¶ä»–åˆ†ç»„å¯åŒ¹é…æ—¶æ‰ä½¿ç”¨ï¼‰
    low_priority_reasons = [
        "é¢„æœŸæ”¹å–„"
    ]

    # æ„å»ºæ¢¯é˜Ÿå›¾
    build_ladder_chart(start_date, end_date, min_board_level=min_board_level,
                       non_main_board_level=non_main_board_level, show_period_change=show_period_change,
                       priority_reasons=priority_reasons, low_priority_reasons=low_priority_reasons,
                       enable_attention_criteria=True, sheet_name=sheet_name,
                       create_leader_sheet=True, create_volume_sheet=True)

    # å¯¼å‡ºè‚¡ç¥¨ä»£ç åˆ°å€™é€‰è‚¡ç¥¨txtæ–‡ä»¶
    from utils.export_stock_codes import extract_stock_codes_from_excel
    from analysis.loader.fupan_data_loader import OUTPUT_FILE

    excel_file = OUTPUT_FILE
    output_txt = "bin/candidate_temp/candidate_stocks.txt"
    print("\n" + "=" * 60)
    extract_stock_codes_from_excel(excel_file, output_txt)
    print("=" * 60 + "\n")


def erban_longtou_analysis():
    """
    äºŒæ¿å®šé¾™å¤´åˆ†æ
    
    åˆ†ææŒ‡å®šæ—¶é—´æ®µå†…äºŒè¿æ¿è‚¡ç¥¨çš„æ™‹çº§ç‡ã€èƒœç‡ã€é¢˜æç‰¹å¾å’Œé‡ä»·å…³ç³»ï¼Œ
    ç”ŸæˆMarkdownæ ¼å¼çš„åˆ†ææŠ¥å‘Šï¼Œå¸®åŠ©ç†è§£å¸‚åœºçƒ­ç‚¹å’Œé¾™å¤´è‚¡ç‰¹å¾ã€‚
    """
    # é…ç½®å‚æ•°
    start_date = '20251001'  # å¼€å§‹æ—¥æœŸ
    end_date = '20251201'  # ç»“æŸæ—¥æœŸï¼ŒNoneè¡¨ç¤ºåˆ°ä»Šå¤©
    min_concept_samples = 2  # é¢˜æç»Ÿè®¡æœ€å°æ ·æœ¬æ•°
    output_path = None  # è¾“å‡ºè·¯å¾„ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨ç”Ÿæˆ

    # æ‰§è¡Œåˆ†æ
    report_path = analyze_erban_longtou(
        start_date=start_date,
        end_date=end_date,
        output_path=output_path,
        min_concept_samples=min_concept_samples
    )

    if report_path:
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    else:
        print("\nâŒ åˆ†æå¤±è´¥ï¼Œæœªç”ŸæˆæŠ¥å‘Š")


def generate_comparison_charts(candidate_model: str = 'a', recent_days: int = 10):
    """
    ç”Ÿæˆè‚¡ç¥¨ä¿¡å·å¯¹æ¯”å›¾ - æ ¹æ®ä¿¡å·æ—¥æœŸåˆ†ç»„ï¼Œä¾¿äºå¯¹æ¯”æŸ¥çœ‹

    Args:
        candidate_model: ä½¿ç”¨çš„å€™é€‰é›†æ¨¡å‹æ ‡è¯†ï¼ˆå¦‚ 'a'ã€'b'ï¼‰ï¼Œç”¨äºåŒºåˆ†è¾“å‡ºç›®å½•
        recent_days: ç”Ÿæˆæœ€è¿‘å‡ å¤©çš„å¯¹æ¯”å›¾ï¼Œé»˜è®¤10å¤©
    """
    from bin.comparison_chart_generator import run_auto_generation
    base_dir = f'bin/candidate_stocks_breakout_{candidate_model}'
    return run_auto_generation(base_dir=base_dir, recent_days=recent_days)


def generate_rebound_comparison_charts(candidate_model: str = 'a', recent_days: int = 10):
    """
    ç”Ÿæˆæ­¢è·Œåå¼¹ç­–ç•¥çš„è‚¡ç¥¨ä¿¡å·å¯¹æ¯”å›¾

    Args:
        candidate_model: ä½¿ç”¨çš„å€™é€‰é›†æ¨¡å‹æ ‡è¯†ï¼ˆå¦‚ 'a'ã€'b'ï¼‰ï¼Œç”¨äºåŒºåˆ†è¾“å‡ºç›®å½•
        recent_days: ç”Ÿæˆæœ€è¿‘å‡ å¤©çš„å¯¹æ¯”å›¾ï¼Œé»˜è®¤10å¤©
    """
    from bin.comparison_chart_generator import run_auto_generation
    base_dir = f'bin/candidate_stocks_rebound_{candidate_model}'
    return run_auto_generation(base_dir=base_dir, recent_days=recent_days)


def auction_fengdan_analyze(date_str: str = None, show_plot: bool = False):
    """
    é›†åˆç«ä»·å°å•æ•°æ®å¤ç›˜åˆ†æ

    Args:
        date_str: æŒ‡å®šåˆ†ææ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼Œé»˜è®¤ä¸ºæœ€è¿‘äº¤æ˜“æ—¥
        show_plot: æ˜¯å¦æ˜¾ç¤ºå›¾è¡¨ï¼Œé»˜è®¤Falseï¼ˆé¿å…é˜»å¡ï¼‰
    """
    from analysis.auction_fengdan_analysis import AuctionFengdanAnalyzer

    analyzer = AuctionFengdanAnalyzer()
    result = analyzer.run_comprehensive_analysis(date_str=date_str, show_plot=show_plot)

    if result:
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“… åˆ†ææ—¥æœŸ: {result['date']}")
        print(f"ğŸ“Š æ¶¨åœ: {result['zt_count']} åªï¼Œè·Œåœ: {result['dt_count']} åªï¼Œç«ä»·å°æ¿: {result['auction_count']} åª")
        if result.get('report_file'):
            print(f"ğŸ“„ åˆ†ææŠ¥å‘Š: {result['report_file']}")
        if result.get('chart_file'):
            print(f"ğŸ“Š åˆ†æå›¾è¡¨: {result['chart_file']}")
    else:
        print("âŒ åˆ†æå¤±è´¥æˆ–æ— æ•°æ®")


def analyze_lianban_stocks(start_date='20250101', end_date='20250131',
                           min_lianban=3, lianban_type=1,
                           before_days=30, after_days=10):
    """
    åˆ†æè¿æ¿è‚¡ç¥¨å¹¶ç”ŸæˆKçº¿å›¾
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - ä»å¤ç›˜æ•°æ®ä¸­ç­›é€‰æŒ‡å®šæ—¶é—´æ®µå†…çš„è¿æ¿è‚¡
    - ä¸ºæ¯åªè‚¡ç¥¨ç”Ÿæˆç‹¬ç«‹çš„Kçº¿å›¾ï¼Œä¾¿äºæ‰¾å‡ºè¿æ¿è‚¡çš„å…±æ€§
    - ç”Ÿæˆæ±‡æ€»æŠ¥å‘ŠCSV
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼Œé»˜è®¤'20250101'
        end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼Œé»˜è®¤'20250131'
        min_lianban: æœ€å°è¿æ¿æ•°ï¼Œé»˜è®¤3
        lianban_type: è¿æ¿ç±»å‹ï¼Œé»˜è®¤1
            - 1: è¿ç»­æ¿ï¼ˆæ— æ–­æ¿ï¼‰- è¿ç»­æ¶¨åœå¤©æ•° >= min_lianban
            - 2: æœ€é«˜æ¿ - æœ€é«˜æ¿æ•° >= min_lianbanï¼ˆå¯ä»¥æœ‰æ–­æ¿ï¼‰
            - 3: éè¿ç»­æ¿ - æœ€é«˜æ¿æ•° >= min_lianban ä¸”æœ‰æ–­æ¿
        before_days: é¦–æ¿å‰æ˜¾ç¤ºçš„äº¤æ˜“æ—¥æ•°ï¼Œé»˜è®¤30
        after_days: ç»ˆæ­¢åæ˜¾ç¤ºçš„äº¤æ˜“æ—¥æ•°ï¼Œé»˜è®¤10
    
    è¾“å‡ºï¼š
        - Kçº¿å›¾ä¿å­˜åœ¨: analysis/pattern_charts/{è¿ç»­æ¿åˆ†æ|æœ€é«˜æ¿åˆ†æ|éè¿ç»­æ¿åˆ†æ}/{start_date}_{end_date}/
        - æ±‡æ€»æŠ¥å‘Š: summary.csv
    """
    from analysis.lianban_pattern_analyzer import LianbanPatternAnalyzer, LianbanPatternConfig

    # åˆ›å»ºé…ç½®
    config = LianbanPatternConfig(
        start_date=start_date,
        end_date=end_date,
        min_lianban_count=min_lianban,
        lianban_type=lianban_type,
        before_days=before_days,
        after_days=after_days
    )

    # æ‰§è¡Œåˆ†æ
    analyzer = LianbanPatternAnalyzer(config)
    analyzer.run()

    print(f"\nâœ… åˆ†æå®Œæˆï¼å…±ç”Ÿæˆ {len(analyzer.filtered_stocks)} å¼ å›¾è¡¨")
    print(f"ğŸ“ å›¾è¡¨ä¿å­˜åœ¨: {analyzer.output_dir}")

    return analyzer.output_dir


def analyze_volume_surge_pattern(start_date='20250101', end_date='20250131',
                                 volume_surge_ratio=2.0, volume_avg_days=5,
                                 min_lianban=2, before_days=30, after_days=10):
    """
    åˆ†æ"çˆ†é‡åˆ†æ­§è½¬ä¸€è‡´"å½¢æ€å¹¶ç”ŸæˆKçº¿å›¾
    
    å½¢æ€å®šä¹‰ï¼š
    - å¼ºåŠ¿è¿æ¿è‚¡åœ¨æŸæ—¥å‡ºç°çˆ†é‡ï¼ˆå½“æ—¥é‡èƒ½è¾ƒè¿‘æœŸæ˜æ˜¾æ”¾å¤§ï¼‰
    - ä½†å½“æ—¥ä»ç„¶ä¸Šæ¶¨ï¼ˆä»Šæ”¶ > æ˜¨æ”¶ï¼Œä¸è¦æ±‚æ¶¨åœï¼‰
    - è¿™ç§å½¢æ€ä»£è¡¨åˆ†æ­§åèµ„é‡‘é€‰æ‹©ç»§ç»­åšå¤š
    
    æ ¸å¿ƒç›®çš„ï¼š
    - å¯»æ‰¾è¿™ç±»å½¢æ€çš„è§„å¾‹
    - è§‚å¯Ÿåç»­èµ°åŠ¿
    - åˆ†æä»€ä¹ˆæ—¶æ®µä»€ä¹ˆå½¢æ€çš„è‚¡ç¥¨èµ„é‡‘æœ€æ„¿æ„ä¹°å…¥
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼Œé»˜è®¤'20250101'
        end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼Œé»˜è®¤'20250131'
        volume_surge_ratio: çˆ†é‡é˜ˆå€¼ï¼ˆå½“æ—¥é‡/å‰Næ—¥å‡é‡ï¼‰ï¼Œé»˜è®¤2.0è¡¨ç¤ºé‡èƒ½ç¿»å€
        volume_avg_days: è®¡ç®—å‡é‡çš„å¤©æ•°ï¼Œé»˜è®¤5å¤©
        min_lianban: æœ€å°è¿æ¿æ•°ï¼Œåªåˆ†æè¾¾åˆ°æ­¤è¿æ¿æ•°çš„è‚¡ç¥¨ï¼Œé»˜è®¤2
        before_days: å½¢æ€æ—¥æœŸå‰æ˜¾ç¤ºçš„äº¤æ˜“æ—¥æ•°ï¼Œé»˜è®¤30
        after_days: å½¢æ€æ—¥æœŸåæ˜¾ç¤ºçš„äº¤æ˜“æ—¥æ•°ï¼Œé»˜è®¤10
    
    è¾“å‡ºï¼š
        - Kçº¿å›¾ä¿å­˜åœ¨: analysis/pattern_charts/çˆ†é‡åˆ†æ­§è½¬ä¸€è‡´/{start_date}_{end_date}/
        - æ±‡æ€»æŠ¥å‘Š: analysis/pattern_charts/çˆ†é‡åˆ†æ­§è½¬ä¸€è‡´/{start_date}_{end_date}/summary.csv
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        # åˆ†æ2025å¹´1æœˆçš„çˆ†é‡åˆ†æ­§å½¢æ€
        analyze_volume_surge_pattern('20250101', '20250131')
        
        # åªåˆ†æ3è¿æ¿ä»¥ä¸Šçš„è‚¡ç¥¨ï¼Œé‡èƒ½æ”¾å¤§1.5å€ä»¥ä¸Šå³è§†ä¸ºçˆ†é‡
        analyze_volume_surge_pattern('20250101', '20250131', volume_surge_ratio=1.5, min_lianban=3)
    """
    from analysis.volume_surge_analyzer import VolumeSurgeAnalyzer, VolumeSurgeConfig

    # åˆ›å»ºé…ç½®
    config = VolumeSurgeConfig(
        start_date=start_date,
        end_date=end_date,
        volume_surge_ratio=volume_surge_ratio,
        volume_avg_days=volume_avg_days,
        min_lianban_count=min_lianban,
        before_days=before_days,
        after_days=after_days
    )

    # æ‰§è¡Œåˆ†æ
    analyzer = VolumeSurgeAnalyzer(config)
    analyzer.run()

    print(f"\nâœ… åˆ†æå®Œæˆï¼å…±ç”Ÿæˆ {len(analyzer.filtered_stocks)} å¼ å›¾è¡¨")
    print(f"ğŸ“ å›¾è¡¨ä¿å­˜åœ¨: {analyzer.output_dir}")

    return analyzer.output_dir


def analyze_gap_up_stocks(start_date='20250101', end_date='20250131',
                          min_gap=1.0, max_gap=6.0,
                          filter_enabled=False,
                          filter_days=5, filter_min_change=10.0, filter_max_change=100.0):
    """
    åˆ†æè·³ç©ºé«˜å¼€è‚¡ç¥¨å¹¶ç”ŸæˆKçº¿å›¾
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - æ‰«æå…¨å¸‚åœºè‚¡ç¥¨ï¼Œå¯»æ‰¾è·³ç©ºé«˜å¼€çš„è‚¡ç¥¨
    - æ”¯æŒå‰æœŸæ¶¨å¹…è¿‡æ»¤
    - åŒä¸€åªè‚¡ç¥¨çš„å¤šæ¬¡è·³ç©ºåˆå¹¶åœ¨ä¸€å¼ å›¾ä¸Š
    - ç”Ÿæˆæ±‡æ€»æŠ¥å‘ŠCSV
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼Œé»˜è®¤'20250101'
        end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼Œé»˜è®¤'20250131'
        min_gap: æœ€å°è·³ç©ºå¹…åº¦ï¼ˆ%ï¼‰ï¼Œé»˜è®¤1.0
        max_gap: æœ€å¤§è·³ç©ºå¹…åº¦ï¼ˆ%ï¼‰ï¼Œé»˜è®¤6.0
        filter_enabled: æ˜¯å¦å¯ç”¨å‰æœŸæ¶¨å¹…è¿‡æ»¤ï¼Œé»˜è®¤False
        filter_days: å‰xä¸ªäº¤æ˜“æ—¥ï¼Œé»˜è®¤5
        filter_min_change: å‰æœŸæœ€å°æ¶¨å¹…ï¼ˆ%ï¼‰ï¼Œé»˜è®¤10.0
        filter_max_change: å‰æœŸæœ€å¤§æ¶¨å¹…ï¼ˆ%ï¼‰ï¼Œé»˜è®¤100.0
    
    æ³¨æ„ï¼š
        - å›¾è¡¨æ—¶é—´èŒƒå›´ç”±å…¨å±€é…ç½®CHART_BEFORE_DAYSå’ŒCHART_AFTER_DAYSæ§åˆ¶
        - å¦‚éœ€ä¿®æ”¹ï¼Œè¯·åœ¨gap_up_analyzer.pyä¸­è°ƒæ•´è¿™ä¸¤ä¸ªå…¨å±€å˜é‡
    
    è¾“å‡ºï¼š
        - Kçº¿å›¾ä¿å­˜åœ¨: analysis/gap_up_charts/{start_date}_{end_date}/
        - æ±‡æ€»æŠ¥å‘Š: analysis/gap_up_charts/{start_date}_{end_date}/summary.csv
    """
    from analysis.gap_up_analyzer import GapUpAnalyzer, GapUpAnalysisConfig

    # åˆ›å»ºé…ç½®
    config = GapUpAnalysisConfig(
        start_date=start_date,
        end_date=end_date,
        min_gap_percent=min_gap,
        max_gap_percent=max_gap,
        filter_enabled=filter_enabled,
        filter_days=filter_days,
        filter_min_change=filter_min_change,
        filter_max_change=filter_max_change
    )

    # æ‰§è¡Œåˆ†æ
    analyzer = GapUpAnalyzer(config)
    analyzer.run()

    # ç»Ÿè®¡ç»“æœ
    from collections import defaultdict
    stock_groups = defaultdict(list)
    for stock_info in analyzer.filtered_stocks:
        key = (stock_info.code, stock_info.name)
        stock_groups[key].append(stock_info)

    print(f"\nâœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ“Š å…± {len(stock_groups)} åªè‚¡ç¥¨ï¼Œ{len(analyzer.filtered_stocks)} æ¬¡è·³ç©ºè®°å½•")
    print(f"ğŸ“ å›¾è¡¨ä¿å­˜åœ¨: {analyzer.output_dir}")

    return analyzer.output_dir


if __name__ == '__main__':
    # === çƒ­é—¨å¤©æ¢¯ ===
    # whimsical_fupan_analyze()
    # generate_ladder_chart()

    # === å¤ç›˜ç›¸å…³ ===
    # get_stock_datas()
    # daily_routine()
    # full_scan_routine()
    # get_index_data()
    # review_history('2025-10-24', '2025-10-27')  # å¯è§†åŒ–candidate_history
    # find_candidate_stocks()
    # find_candidate_stocks_weekly_growth(offset_days=0)
    # strategy_scan('a')
    # generate_comparison_charts('a')
    # batch_analyze_weekly_growth_win_rate()
    # pullback_rebound_scan('a')  # æ­¢è·Œåå¼¹ç­–ç•¥æ‰«æ
    # generate_rebound_comparison_charts('a')
    # fetch_ths_fupan()

    # === è¿æ¿è‚¡åˆ†æå›¾åŠŸèƒ½ ===
    # analyze_lianban_stocks('20251101', '20251222', min_lianban=3, lianban_type=1)  # è¿ç»­æ¿åˆ†æ
    analyze_volume_surge_pattern('20251130', '20251222', min_lianban=1, volume_surge_ratio=3.0, volume_avg_days=3)  # çˆ†é‡åˆ†æ­§åˆ†æ

    # === äºŒæ¿å®šé¾™å¤´åˆ†æ ===
    # erban_longtou_analysis()  # åˆ†æäºŒæ¿è‚¡ç¥¨çš„æ™‹çº§ç‡ã€èƒœç‡å’Œç‰¹å¾

    # === è·³ç©ºé«˜å¼€è‚¡ç¥¨åˆ†æåŠŸèƒ½ ===
    # analyze_gap_up_stocks('20250901', '20251029', min_gap=2.0, max_gap=6.0, filter_enabled=True,
    #                       filter_days=20, filter_min_change=-20.0, filter_max_change=20.0)  # è·³ç©ºåˆ†æ

    # === å¤ç›˜å›¾ç”Ÿæˆ ===
    # draw_ths_fupan()        # PNGé™æ€å›¾
    # draw_ths_fupan_html()     # HTMLäº¤äº’å›¾

    # === åŒä¹‰è¯ç®¡ç† ===
    # update_synonym_groups()  # æ·»åŠ æ–°è¯
    # clean_synonym_groups()  # æ¸…ç†æ—§è¯
    # fupan_statistics_to_excel()
    # fupan_statistics_excel_plot()
    # find_yidong()
    # daily_group_analyze()
    # analyze_advanced_on()
    # stocks_time_sharing_price()
    # plot_stock_daily_prices()
    # get_hot_clouds('20251201')
    # find_dragon()
    # find_similar_trends()
    # get_stock_concept_and_industry()
    # fetch_and_filter_top_yybph()
    # get_top_yyb_trades()
    # get_lhb_datas()
    # get_stock_minute_datas()
    # check_stock_datas()

    # === ç­–ç•¥å›æµ‹ ===
    # backtrade_simulate()
    # pullback_rebound_simulate()  # æ­¢è·Œåå¼¹ç­–ç•¥å›æµ‹
    # weekly_volume_momentum_simulate()  # æ‰¬å¸†èµ·èˆªç­–ç•¥å›æµ‹
    # run_psq_analysis()

    # === å¤§æ‰¹é‡å›æµ‹ï¼ˆæ–°åŠŸèƒ½ï¼‰===
    # generate_stock_lists()  # ç”Ÿæˆå…¨éƒ¨Aè‚¡åˆ—è¡¨æ–‡ä»¶ï¼ˆé¦–æ¬¡ä½¿ç”¨å‰è¿è¡Œä¸€æ¬¡ï¼‰
    # generate_fupan_candidates()  # ä»å¤ç›˜æ•°æ®æå–çƒ­é—¨è‚¡å€™é€‰ï¼ˆå¯åå¤è¿è¡Œï¼‰
    # batch_backtest_from_stock_list()  # ä»æ–‡ä»¶è¯»å–è‚¡ç¥¨åˆ—è¡¨è¿›è¡Œæ‰¹é‡å›æµ‹
    # batch_backtest_from_codes()  # ç›´æ¥ä½¿ç”¨ä»£ç åˆ—è¡¨è¿›è¡Œæ‰¹é‡å›æµ‹

    # === å‚æ•°ä¼˜åŒ–åŠŸèƒ½ ===
    # 1. ç”Ÿæˆé…ç½®æ¨¡æ¿
    # generate_optimization_templates()
    # 2. è¿è¡Œå‚æ•°ä¼˜åŒ–ï¼ˆéœ€è¦å…ˆç”Ÿæˆå¹¶ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼‰
    # run_parameter_optimization("compare_config.yaml")

    # === é›†åˆç«ä»·å°å•æ•°æ®åŠŸèƒ½ ===
    # auction_fengdan_analyze()  # å¤ç›˜åˆ†æå°å•æ•°æ®
    # å®šæ—¶é‡‡é›†è¯·è¿è¡Œ: python alerting/auction_scheduler.py start
